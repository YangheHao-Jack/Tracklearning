{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Dataset and Transforms\n",
    "# ------------------------------\n",
    "\n",
    "class ResizeAndToTensor:\n",
    "    \"\"\"\n",
    "    Custom transform to resize images and masks to a fixed size and convert them to tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(512, 512)):\n",
    "        self.size = size  # (height, width)\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL Image): Input image.\n",
    "            mask (np.array): Corresponding mask as a NumPy array.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor (torch.Tensor): Resized image tensor.\n",
    "            mask_tensor (torch.Tensor): Resized mask tensor.\n",
    "        \"\"\"\n",
    "        # Resize image using bilinear interpolation\n",
    "        image = resize(\n",
    "            image,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.BILINEAR\n",
    "        )\n",
    "        # Resize mask using nearest-neighbor interpolation to preserve label integrity\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        mask_pil = resize(\n",
    "            mask_pil,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.NEAREST\n",
    "        )\n",
    "        # Convert back to NumPy array\n",
    "        mask = np.array(mask_pil, dtype=np.uint8)\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image_tensor = to_tensor(image)             # [C, H, W], float32 in [0,1]\n",
    "        mask_tensor  = torch.from_numpy(mask).long()# [H, W], dtype=torch.int64\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "class BinarySegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for binary image segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, file_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with input images.\n",
    "            masks_dir (str): Directory with corresponding mask files (.npy).\n",
    "            file_list (list): List of image filenames.\n",
    "            transform (callable, optional): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir  = masks_dir\n",
    "        self.file_list  = file_list\n",
    "        self.transform  = transform\n",
    "\n",
    "        # Debug: Print how many files in this split\n",
    "        print(f\"[Dataset] Initialized with {len(file_list)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask pair at the specified index.\n",
    "        \"\"\"\n",
    "        # Debug: Uncomment for detailed logs\n",
    "        # print(f\"[Dataset] __getitem__ called with idx={idx}\")\n",
    "        \n",
    "        img_filename = self.file_list[idx]\n",
    "        base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"[Error] Image file not found: {img_path}\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.masks_dir, base_name + \".npy\")\n",
    "        if not os.path.isfile(mask_path):\n",
    "            raise FileNotFoundError(f\"[Error] Mask file not found: {mask_path}\")\n",
    "        mask = np.load(mask_path)\n",
    "\n",
    "        # Remap any label >1 to 1 to ensure binary segmentation\n",
    "        mask = np.where(mask > 1, 1, mask).astype(np.uint8)\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ------------------------------\n",
    "# 2. U-Net Model Definition\n",
    "# ------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two convolutional layers each followed by BatchNorm and ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture for image segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels (e.g., 3 for RGB).\n",
    "            out_channels (int): Number of output channels/classes.\n",
    "        \"\"\"\n",
    "        super(UNet, self).__init__()\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.conv4 = DoubleConv(1024, 512)\n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.conv3 = DoubleConv(512, 256)\n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.conv2 = DoubleConv(256, 128)\n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.conv1 = DoubleConv(128, 64)\n",
    "\n",
    "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the U-Net.\n",
    "        \"\"\"\n",
    "        d1 = self.down1(x)\n",
    "        p1 = self.pool1(d1)\n",
    "        d2 = self.down2(p1)\n",
    "        p2 = self.pool2(d2)\n",
    "        d3 = self.down3(p2)\n",
    "        p3 = self.pool3(d3)\n",
    "        d4 = self.down4(p3)\n",
    "        p4 = self.pool4(d4)\n",
    "\n",
    "        bn = self.bottleneck(p4)\n",
    "\n",
    "        up_4 = self.up4(bn)\n",
    "        merge4 = torch.cat([d4, up_4], dim=1)\n",
    "        c4 = self.conv4(merge4)\n",
    "\n",
    "        up_3 = self.up3(c4)\n",
    "        merge3 = torch.cat([d3, up_3], dim=1)\n",
    "        c3 = self.conv3(merge3)\n",
    "\n",
    "        up_2 = self.up2(c3)\n",
    "        merge2 = torch.cat([d2, up_2], dim=1)\n",
    "        c2 = self.conv2(merge2)\n",
    "\n",
    "        up_1 = self.up1(c2)\n",
    "        merge1 = torch.cat([d1, up_1], dim=1)\n",
    "        c1 = self.conv1(merge1)\n",
    "\n",
    "        out = self.out(c1)\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Training and Validation Functions\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch_idx):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Train] Starting epoch {epoch_idx+1} ---\")\n",
    "    # Use tqdm to show progress\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Train] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, epoch_idx):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Val] Starting epoch {epoch_idx+1} ---\")\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Val] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Testing and Metrics\n",
    "# ------------------------------\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient for binary masks.\n",
    "    \n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted mask [H, W].\n",
    "        target (torch.Tensor): Ground truth mask [H, W].\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    \n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1).float()\n",
    "    target_flat = target.view(-1).float()\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def test_segmentation(model, loader, device):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and computes the average Dice coefficient.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): Device to run on.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average Dice coefficient on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(loader, desc=\"Testing\"):\n",
    "            images = images.to(device)\n",
    "            masks  = masks.to(device)  # [B, H, W]\n",
    "\n",
    "            outputs = model(images)    # [B, 2, H, W]\n",
    "            preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            for pred, mask in zip(preds, masks):\n",
    "                dice = dice_coefficient(pred, mask)\n",
    "                dice_scores.append(dice)\n",
    "    \n",
    "    mean_dice = np.mean(dice_scores) if dice_scores else 0\n",
    "    return mean_dice\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=3):\n",
    "    \"\"\"\n",
    "    Visualizes predictions alongside the original images and ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        dataset (Dataset): The test dataset.\n",
    "        device (torch.device): Device to run on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_batch = image.unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(image_batch)  # [1, 2, H, W]\n",
    "            pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "        \n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "        mask_np = mask.cpu().numpy()  # [H, W]\n",
    "        \n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        # Handle image range for visualization\n",
    "        if image_np.max() > 1:\n",
    "            image_np = image_np.astype(np.uint8)\n",
    "        else:\n",
    "            image_np = (image_np * 255).astype(np.uint8)\n",
    "        \n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "        \n",
    "        axs[1].imshow(mask_np, cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "        \n",
    "        axs[2].imshow(pred, cmap='gray')\n",
    "        axs[2].set_title(\"Predicted Mask\")\n",
    "        axs[2].axis(\"off\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Main Function\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # Paths Configuration\n",
    "    # ------------------------------\n",
    "    # Define your directories here\n",
    "    # Training and Validation directories (split from the same folder)\n",
    "    images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_train/images/\"\n",
    "    masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_train/masks/\"\n",
    "\n",
    "    # Testing directories (completely separate)\n",
    "    test_images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_test/images/\"\n",
    "    test_masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_test/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size    = 4\n",
    "    num_epochs    = 10\n",
    "    learning_rate = 1e-4\n",
    "\n",
    "    # ------------------------------\n",
    "    # Device Configuration\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"[Main] Using device:\", device)\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect File Lists\n",
    "    # ------------------------------\n",
    "    all_train_images = sorted([\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_images)} training image files in {images_dir}\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training image files found. Check your training path!\")\n",
    "        return\n",
    "\n",
    "    all_train_masks = sorted([\n",
    "        f for f in os.listdir(masks_dir)\n",
    "        if f.lower().endswith(\".npy\")\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_masks)} training mask files in {masks_dir}\")\n",
    "\n",
    "    if len(all_train_masks) == 0:\n",
    "        print(\"[Error] No training mask files found. Check your training mask path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure that the number of images and masks match\n",
    "    if len(all_train_images) != len(all_train_masks):\n",
    "        print(\"[Error] Number of training images and masks do not match!\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------\n",
    "    # Train/Validation Split\n",
    "    # ------------------------------\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_train_images, \n",
    "        test_size=0.2,  # 20% for validation\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"[Main] Training samples: {len(train_files)}\")\n",
    "    print(f\"[Main] Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Datasets\n",
    "    # ------------------------------\n",
    "    transform = ResizeAndToTensor(size=(512, 512))\n",
    "\n",
    "    train_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=train_files,\n",
    "        transform=transform\n",
    "    )\n",
    "    val_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=val_files,\n",
    "        transform=transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create DataLoaders\n",
    "    # ------------------------------\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Test Dataset and DataLoader\n",
    "    # ------------------------------\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(test_images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_images)} test image files in {test_images_dir}\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test image files found. Check your test path!\")\n",
    "        return\n",
    "\n",
    "    all_test_masks = sorted([\n",
    "        f for f in os.listdir(test_masks_dir)\n",
    "        if f.lower().endswith(\".npy\")\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_masks)} test mask files in {test_masks_dir}\")\n",
    "\n",
    "    if len(all_test_masks) == 0:\n",
    "        print(\"[Error] No test mask files found. Check your test mask path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure that the number of test images and masks match\n",
    "    if len(all_test_images) != len(all_test_masks):\n",
    "        print(\"[Error] Number of test images and masks do not match!\")\n",
    "        return\n",
    "\n",
    "    test_dataset = BinarySegDataset(\n",
    "        images_dir=test_images_dir,\n",
    "        masks_dir=test_masks_dir,\n",
    "        file_list=all_test_images,\n",
    "        transform=transform\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    print(f\"[Main] Test samples: {len(test_dataset)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Initialize Model, Loss, Optimizer, Scheduler\n",
    "    # ------------------------------\n",
    "    model = UNet(in_channels=3, out_channels=2).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, \n",
    "        mode='min', \n",
    "        factor=0.5, \n",
    "        patience=3, \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "        val_loss   = validate_one_epoch(model, val_loader, criterion, device, epoch)\n",
    "        \n",
    "        # Step scheduler with validation loss\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {current_lr:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save the model if validation loss has decreased\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(f\">> Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    print(f\"\\nTraining complete. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(\"Best model saved as 'best_model.pth'.\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Testing\n",
    "    # ------------------------------\n",
    "    print(\"\\n>>> Loading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "\n",
    "    test_dice = test_segmentation(model, test_loader, device)\n",
    "    print(f\"Test Dice Coefficient: {test_dice:.4f}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Visualization of Test Samples\n",
    "    # ------------------------------\n",
    "    if len(test_dataset) > 0:\n",
    "        print(\"\\n>>> Visualizing predictions on test samples...\")\n",
    "        visualize_predictions(model, test_dataset, device, num_samples=3)\n",
    "    else:\n",
    "        print(\"[Warning] No test samples available for visualization.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Main] Using device: cuda\n",
      "[Main] Found 14737 training image files in /home/yanghehao/tracklearning/segmentation/phantom_train/images/\n",
      "[Main] 14737 training images have corresponding masks.\n",
      "[Main] Found 3685 test image files in /home/yanghehao/tracklearning/segmentation/phantom_test/images/\n",
      "[Main] 3685 test images have corresponding masks.\n",
      "[Main] Training samples: 11789\n",
      "[Main] Validation samples: 2948\n",
      "[Dataset] Initialized with 11789 files.\n",
      "[Dataset] Initialized with 2948 files.\n",
      "[Dataset] Initialized with 3685 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yanghehao/anaconda3/envs/rr-0.4.2/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/yanghehao/anaconda3/envs/rr-0.4.2/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet34_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet34_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Train] Starting epoch 1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 3/1474 [00:00<06:43,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 0, Loss: 0.8693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|          | 11/1474 [00:01<03:12,  7.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 10, Loss: 0.3038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   1%|▏         | 21/1474 [00:03<02:36,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 20, Loss: 0.1870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 32/1474 [00:04<02:49,  8.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 30, Loss: 0.1338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   3%|▎         | 42/1474 [00:05<02:55,  8.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 40, Loss: 0.1068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▎         | 52/1474 [00:06<02:30,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 50, Loss: 0.0834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|▍         | 63/1474 [00:07<02:39,  8.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 60, Loss: 0.0698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   5%|▍         | 71/1474 [00:08<02:47,  8.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 70, Loss: 0.0598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 83/1474 [00:10<02:33,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 80, Loss: 0.0533\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|▌         | 91/1474 [00:11<02:35,  8.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 90, Loss: 0.0411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   7%|▋         | 103/1474 [00:12<02:42,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 100, Loss: 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 111/1474 [00:13<02:46,  8.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 110, Loss: 0.0369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|▊         | 123/1474 [00:15<02:52,  7.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 120, Loss: 0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   9%|▉         | 132/1474 [00:16<02:21,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 130, Loss: 0.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|▉         | 142/1474 [00:17<02:08, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 140, Loss: 0.0326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|█         | 152/1474 [00:18<02:33,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 150, Loss: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  11%|█         | 163/1474 [00:20<02:40,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 160, Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 171/1474 [00:21<02:37,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 170, Loss: 0.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|█▏        | 183/1474 [00:22<02:37,  8.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 180, Loss: 0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 191/1474 [00:23<02:28,  8.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 190, Loss: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 203/1474 [00:24<02:22,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 200, Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|█▍        | 211/1474 [00:26<02:35,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 210, Loss: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|█▌        | 223/1474 [00:27<02:27,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 220, Loss: 0.0208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▌        | 231/1474 [00:28<02:16,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 230, Loss: 0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|█▋        | 243/1474 [00:29<02:17,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 240, Loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  17%|█▋        | 251/1474 [00:30<02:24,  8.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 250, Loss: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 261/1474 [00:32<02:35,  7.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 260, Loss: 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|█▊        | 271/1474 [00:33<02:20,  8.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 270, Loss: 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  19%|█▉        | 283/1474 [00:34<02:23,  8.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 280, Loss: 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|█▉        | 291/1474 [00:35<02:20,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 290, Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 301/1474 [00:36<02:04,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 300, Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  21%|██        | 311/1474 [00:38<02:16,  8.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 310, Loss: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|██▏       | 321/1474 [00:39<02:20,  8.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 320, Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 333/1474 [00:40<02:03,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 330, Loss: 0.0189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  23%|██▎       | 342/1474 [00:41<02:17,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 340, Loss: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  24%|██▍       | 352/1474 [00:43<02:23,  7.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 350, Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▍       | 362/1474 [00:44<02:12,  8.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 360, Loss: 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  25%|██▌       | 372/1474 [00:45<02:10,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 370, Loss: 0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|██▌       | 380/1474 [00:46<02:10,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 380, Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 392/1474 [00:48<02:15,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 390, Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  27%|██▋       | 400/1474 [00:49<02:09,  8.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 400, Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  28%|██▊       | 410/1474 [00:50<02:08,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 410, Loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▊       | 422/1474 [00:51<02:03,  8.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 420, Loss: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  29%|██▉       | 430/1474 [00:53<02:06,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 430, Loss: 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|██▉       | 442/1474 [00:54<02:07,  8.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 440, Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 452/1474 [00:55<01:56,  8.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 450, Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███▏      | 462/1474 [00:56<01:59,  8.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 460, Loss: 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|███▏      | 470/1474 [00:57<01:48,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 470, Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 482/1474 [00:59<02:04,  7.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 480, Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  33%|███▎      | 492/1474 [01:00<02:04,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 490, Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  34%|███▍      | 501/1474 [01:01<01:49,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 500, Loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▍      | 513/1474 [01:03<01:58,  8.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 510, Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  35%|███▌      | 521/1474 [01:04<01:52,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 520, Loss: 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  36%|███▌      | 533/1474 [01:05<01:53,  8.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 530, Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  37%|███▋      | 542/1474 [01:06<01:44,  8.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 540, Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 553/1474 [01:08<01:56,  7.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 550, Loss: 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|███▊      | 561/1474 [01:09<01:52,  8.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 560, Loss: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 573/1474 [01:10<01:47,  8.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 570, Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  39%|███▉      | 581/1474 [01:11<01:37,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 580, Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 593/1474 [01:13<01:44,  8.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 590, Loss: 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|████      | 601/1474 [01:14<01:42,  8.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 600, Loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 613/1474 [01:15<01:37,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 610, Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  42%|████▏     | 621/1474 [01:16<01:41,  8.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 620, Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 633/1474 [01:18<01:40,  8.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 630, Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  43%|████▎     | 641/1474 [01:19<01:20, 10.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 640, Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|████▍     | 653/1474 [01:20<01:33,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 650, Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  45%|████▍     | 661/1474 [01:21<01:43,  7.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 660, Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 673/1474 [01:23<01:32,  8.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 670, Loss: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  46%|████▌     | 681/1474 [01:24<01:37,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 680, Loss: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  47%|████▋     | 693/1474 [01:25<01:30,  8.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 690, Loss: 0.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 702/1474 [01:27<01:36,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 700, Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|████▊     | 712/1474 [01:28<01:36,  7.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 710, Loss: 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  49%|████▉     | 722/1474 [01:29<01:33,  8.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 720, Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|████▉     | 730/1474 [01:30<01:27,  8.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 730, Loss: 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|█████     | 742/1474 [01:31<01:15,  9.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 740, Loss: 0.0098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  51%|█████     | 750/1474 [01:32<01:23,  8.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 750, Loss: 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 762/1474 [01:34<01:24,  8.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 760, Loss: 0.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|█████▏    | 770/1474 [01:35<01:25,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 770, Loss: 0.0091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  53%|█████▎    | 782/1474 [01:36<01:17,  8.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 780, Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▎    | 790/1474 [01:37<01:23,  8.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 790, Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  54%|█████▍    | 802/1474 [01:39<01:19,  8.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 800, Loss: 0.0097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  55%|█████▌    | 811/1474 [01:40<01:20,  8.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 810, Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▌    | 823/1474 [01:41<01:15,  8.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 820, Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|█████▋    | 831/1474 [01:43<01:16,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 830, Loss: 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  57%|█████▋    | 843/1474 [01:44<01:16,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 840, Loss: 0.0096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 851/1474 [01:45<01:17,  8.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 850, Loss: 0.0100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  58%|█████▊    | 861/1474 [01:46<01:13,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 860, Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  59%|█████▉    | 871/1474 [01:48<01:08,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 870, Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|█████▉    | 883/1474 [01:49<01:10,  8.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 880, Loss: 0.0106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|██████    | 891/1474 [01:50<01:12,  8.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 890, Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  61%|██████▏   | 903/1474 [01:51<01:01,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 900, Loss: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  62%|██████▏   | 911/1474 [01:53<01:07,  8.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 910, Loss: 0.0093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  63%|██████▎   | 923/1474 [01:54<01:14,  7.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Batch 920, Loss: 0.0085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 788\u001b[0m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;66;03m# 7. Run the Main Function\u001b[39;00m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 788\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 737\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    734\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m--> 737\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[1;32m    740\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m validate_one_epoch(model, val_loader, criterion, device, epoch, metrics_val)\n",
      "Cell \u001b[0;32mIn[2], line 363\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, loader, optimizer, criterion, device, epoch_idx)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- [Train] Starting epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_idx\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Use tqdm to show progress\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (images, masks) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[1;32m    364\u001b[0m     images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    365\u001b[0m     masks \u001b[38;5;241m=\u001b[39m masks\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/rr-0.4.2/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rr-0.4.2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/rr-0.4.2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rr-0.4.2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/anaconda3/envs/rr-0.4.2/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/rr-0.4.2/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m~/anaconda3/envs/rr-0.4.2/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models  # For ResNet encoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchmetrics\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# 0. Reproducibility (Optional)\n",
    "# ------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Ensures that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Dataset and Transforms\n",
    "# ------------------------------\n",
    "\n",
    "class ResizeAndToTensor:\n",
    "    \"\"\"\n",
    "    Custom transform to resize images and masks to a fixed size and convert them to tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(512, 512), augment=False):\n",
    "        self.size = size  # (height, width)\n",
    "        self.augment = augment\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL Image): Input image.\n",
    "            mask (np.array or PIL Image): Corresponding mask as a NumPy array or PIL Image.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor (torch.Tensor): Resized and normalized image tensor.\n",
    "            mask_tensor (torch.Tensor): Resized mask tensor (Long type).\n",
    "        \"\"\"\n",
    "        # Resize image using bilinear interpolation\n",
    "        image = resize(\n",
    "            image,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.BILINEAR\n",
    "        )\n",
    "        # Resize mask using nearest-neighbor interpolation to preserve label integrity\n",
    "        if isinstance(mask, np.ndarray):\n",
    "            mask_pil = Image.fromarray(mask)\n",
    "        elif isinstance(mask, Image.Image):\n",
    "            mask_pil = mask\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported mask type: {type(mask)}\")\n",
    "        \n",
    "        mask_pil = resize(\n",
    "            mask_pil,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.NEAREST\n",
    "        )\n",
    "        # Convert back to NumPy array if it was initially\n",
    "        if isinstance(mask, np.ndarray):\n",
    "            mask = np.array(mask_pil, dtype=np.uint8)\n",
    "        else:\n",
    "            mask = np.array(mask_pil, dtype=np.uint8)\n",
    "\n",
    "        # Data Augmentation (if enabled)\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if random.random() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                mask = Image.fromarray(mask).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "            # Random vertical flip\n",
    "            if random.random() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                mask = Image.fromarray(mask).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "            # Random rotation by 0°, 90°, 180°, or 270°\n",
    "            angles = [0, 90, 180, 270]\n",
    "            angle = random.choice(angles)\n",
    "            if angle != 0:\n",
    "                image = image.rotate(angle)\n",
    "                mask = Image.fromarray(mask).rotate(angle)\n",
    "                mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "            # Add more augmentations as needed\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image_tensor = to_tensor(image)             # [C, H, W], float32 in [0,1]\n",
    "        image_tensor = self.normalize(image_tensor)\n",
    "\n",
    "        mask_tensor  = torch.from_numpy(mask).long()# [H, W], dtype=torch.int64\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "class BinarySegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for binary image segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, file_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with input images.\n",
    "            masks_dir (str): Directory with corresponding mask files (.npy or image files).\n",
    "            file_list (list): List of image filenames.\n",
    "            transform (callable, optional): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir  = masks_dir\n",
    "        self.file_list  = file_list\n",
    "        self.transform  = transform\n",
    "\n",
    "        # Debug: Print how many files in this split\n",
    "        print(f\"[Dataset] Initialized with {len(file_list)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask pair at the specified index.\n",
    "        \"\"\"\n",
    "        img_filename = self.file_list[idx]\n",
    "        base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"[Error] Image file not found: {img_path}\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        # Attempt to find mask with the same base name and supported extensions\n",
    "        mask_extensions = ['.npy', '.png', '.jpg', '.jpeg']\n",
    "        mask_path = None\n",
    "        for ext in mask_extensions:\n",
    "            potential_path = os.path.join(self.masks_dir, base_name + ext)\n",
    "            if os.path.isfile(potential_path):\n",
    "                mask_path = potential_path\n",
    "                break\n",
    "        if mask_path is None:\n",
    "            raise FileNotFoundError(f\"[Error] Mask file not found for image: {img_filename}\")\n",
    "\n",
    "        # Load mask\n",
    "        if mask_path.endswith('.npy'):\n",
    "            mask = np.load(mask_path)\n",
    "            mask = Image.fromarray(mask)\n",
    "        else:\n",
    "            mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Remap any label >1 to 1 to ensure binary segmentation\n",
    "        mask_np = np.array(mask)\n",
    "        mask_np = np.where(mask_np > 1, 1, mask_np).astype(np.uint8)\n",
    "        mask = Image.fromarray(mask_np)\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ------------------------------\n",
    "# 2. U-Net Model Definition with ResNet Encoder\n",
    "# ------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two convolutional layers each followed by BatchNorm and ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture with ResNet encoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=2, encoder_name='resnet34', pretrained=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            encoder_name (str): Name of the ResNet encoder to use ('resnet18', 'resnet34', 'resnet50', etc.).\n",
    "            pretrained (bool): Whether to use pretrained ResNet weights.\n",
    "        \"\"\"\n",
    "        super(ResNetUNet, self).__init__()\n",
    "        \n",
    "        # Initialize ResNet encoder\n",
    "        if encoder_name == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet34':\n",
    "            self.encoder = models.resnet34(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet101':\n",
    "            self.encoder = models.resnet101(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet variant\")\n",
    "\n",
    "        # Encoder layers\n",
    "        self.initial = nn.Sequential(\n",
    "            self.encoder.conv1,  # [B, 64, H/2, W/2]\n",
    "            self.encoder.bn1,\n",
    "            self.encoder.relu,\n",
    "            self.encoder.maxpool  # [B, 64, H/4, W/4]\n",
    "        )\n",
    "        self.encoder_layer1 = self.encoder.layer1  # [B, 64 or 256, H/4, W/4]\n",
    "        self.encoder_layer2 = self.encoder.layer2  # [B, 128 or 512, H/8, W/8]\n",
    "        self.encoder_layer3 = self.encoder.layer3  # [B, 256 or 1024, H/16, W/16]\n",
    "        self.encoder_layer4 = self.encoder.layer4  # [B, 512 or 2048, H/32, W/32]\n",
    "\n",
    "        # Decoder layers\n",
    "        self.up4 = nn.ConvTranspose2d(encoder_channels[4], encoder_channels[3], kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(encoder_channels[3] + encoder_channels[3], encoder_channels[3])\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(encoder_channels[3], encoder_channels[2], kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(encoder_channels[2] + encoder_channels[2], encoder_channels[2])\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(encoder_channels[2], encoder_channels[1], kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(encoder_channels[1] + encoder_channels[1], encoder_channels[1])\n",
    "\n",
    "        # Removed up1 and conv1 to prevent spatial dimension mismatch\n",
    "        # If you need more upsampling steps, ensure corresponding encoder layers are present\n",
    "\n",
    "        self.out_conv = nn.Conv2d(encoder_channels[1], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.initial(x)           # [B, 64, H/4, W/4]\n",
    "        x1 = self.encoder_layer1(x0)   # [B, 64, H/4, W/4] for resnet34\n",
    "        x2 = self.encoder_layer2(x1)   # [B, 128, H/8, W/8]\n",
    "        x3 = self.encoder_layer3(x2)   # [B, 256, H/16, W/16]\n",
    "        x4 = self.encoder_layer4(x3)   # [B, 512, H/32, W/32]\n",
    "\n",
    "        # Decoder\n",
    "        up4 = self.up4(x4)             # [B, 256, H/16, W/16]\n",
    "        merge4 = torch.cat([up4, x3], dim=1)  # [B, 512, H/16, W/16]\n",
    "        conv4 = self.conv4(merge4)     # [B, 256, H/16, W/16]\n",
    "\n",
    "        up3 = self.up3(conv4)          # [B, 128, H/8, W/8]\n",
    "        merge3 = torch.cat([up3, x2], dim=1)  # [B, 256, H/8, W/8]\n",
    "        conv3 = self.conv3(merge3)     # [B, 128, H/8, W/8]\n",
    "\n",
    "        up2 = self.up2(conv3)          # [B, 64, H/4, W/4]\n",
    "        merge2 = torch.cat([up2, x1], dim=1)  # [B, 128, H/4, W/4]\n",
    "        conv2 = self.conv2(merge2)     # [B, 64, H/4, W/4]\n",
    "\n",
    "        # No up1 and conv1 to prevent spatial dimension mismatch\n",
    "        out = self.out_conv(conv2)     # [B, n_classes, H/4, W/4]\n",
    "\n",
    "        # Upsample to original size\n",
    "        out = nn.functional.interpolate(out, scale_factor=4, mode='bilinear', align_corners=True)  # [B, n_classes, H, W]\n",
    "\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Early Stopping Class\n",
    "# ------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Initial best loss: {self.best_loss:.4f}\")\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"[EarlyStopping] Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Validation loss improved to: {self.best_loss:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Training and Validation Functions\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch_idx):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Train] Starting epoch {epoch_idx+1} ---\")\n",
    "    # Use tqdm to show progress\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Train] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Train] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    return average_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, epoch_idx, metrics=None):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics to compute.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Val] Starting epoch {epoch_idx+1} ---\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Val] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "    \n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Val] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    \n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        metric_values = {}\n",
    "    \n",
    "    # Optionally, print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Val] Epoch {epoch_idx+1} Metrics: {metric_str} ---\")\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Testing and Metrics\n",
    "# ------------------------------\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient for binary masks.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted mask [H, W].\n",
    "        target (torch.Tensor): Ground truth mask [H, W].\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1).float()\n",
    "    target_flat = target.view(-1).float()\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def test_segmentation(model, loader, device, metrics=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and computes the average Dice coefficient and other metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): Device to run on.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of average metrics on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    iou_scores = []\n",
    "    accuracy_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Testing\", leave=False)):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)  # [B, H, W]\n",
    "\n",
    "            outputs = model(images)    # [B, n_classes, H, W]\n",
    "            preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        # If metrics are not provided, compute only Dice\n",
    "        dice_scores = []\n",
    "        for pred, mask in zip(preds, masks):\n",
    "            dice = dice_coefficient(pred, mask)\n",
    "            dice_scores.append(dice)\n",
    "        metric_values = {\n",
    "            'dice': np.mean(dice_scores) if dice_scores else 0\n",
    "        }\n",
    "\n",
    "    # Print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Test] Metrics: {metric_str} ---\")\n",
    "    else:\n",
    "        print(f\"--- [Test] Dice Coefficient: {metric_values['dice']:.4f} ---\")\n",
    "\n",
    "    return metric_values\n",
    "\n",
    "\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=3, post_process_flag=False):\n",
    "    \"\"\"\n",
    "    Visualizes predictions alongside the original images and ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        dataset (Dataset): The test dataset.\n",
    "        device (torch.device): Device to run on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        post_process_flag (bool): Whether to apply post-processing to predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_batch = image.unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image_batch)  # [1, n_classes, H, W]\n",
    "            pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "\n",
    "\n",
    "\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "        mask_np = mask.cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Handle image normalization for visualization\n",
    "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "        image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(mask_np, cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(pred, cmap='gray')\n",
    "        axs[2].set_title(\"Predicted Mask\" + (\" (Post-Processed)\" if post_process_flag else \"\"))\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Main Function\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # Define Your Directories Here\n",
    "    # ------------------------------\n",
    "    # Training and Validation directories (split from the same folder)\n",
    "    images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_train/images/\"\n",
    "    masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_train/masks/\"\n",
    "\n",
    "    # Testing directories (completely separate)\n",
    "    test_images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_test/images/\"\n",
    "    test_masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_test/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size    = 8\n",
    "    num_epochs    = 20\n",
    "    learning_rate = 1e-3\n",
    "    val_split     = 0.2\n",
    "    save_path     = \"best_model.pth\"\n",
    "    patience      = 5\n",
    "    post_process_flag = True  # Set to True to apply post-processing\n",
    "\n",
    "    # ------------------------------\n",
    "    # Device Configuration\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Main] Using device: {device}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect Training File Lists\n",
    "    # ------------------------------\n",
    "    # List all training image files\n",
    "    all_train_images = sorted([\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_images)} training image files in {images_dir}\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training image files found. Check your training path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding mask files exist\n",
    "    all_train_images = sorted([\n",
    "        f for f in all_train_images\n",
    "        if any(\n",
    "            os.path.isfile(os.path.join(masks_dir, os.path.splitext(f)[0] + ext))\n",
    "            for ext in ['.npy', '.png', '.jpg', '.jpeg']\n",
    "        )\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_train_images)} training images have corresponding masks.\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training mask files found or mismatched filenames. Check your training mask path!\")\n",
    "        return\n",
    "\n",
    "    # List all test image files\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(test_images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_images)} test image files in {test_images_dir}\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test image files found. Check your test path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding test mask files exist\n",
    "    all_test_images = sorted([\n",
    "        f for f in all_test_images\n",
    "        if any(\n",
    "            os.path.isfile(os.path.join(test_masks_dir, os.path.splitext(f)[0] + ext))\n",
    "            for ext in ['.npy', '.png', '.jpg', '.jpeg']\n",
    "        )\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_test_images)} test images have corresponding masks.\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test mask files found or mismatched filenames. Check your test mask path!\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------\n",
    "    # Train/Validation Split\n",
    "    # ------------------------------\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_train_images,\n",
    "        test_size=val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"[Main] Training samples: {len(train_files)}\")\n",
    "    print(f\"[Main] Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Datasets with Transforms\n",
    "    # ------------------------------\n",
    "    train_transform = ResizeAndToTensor(size=(512, 512), augment=True)\n",
    "    val_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "    test_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "\n",
    "    train_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=train_files,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=val_files,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = BinarySegDataset(\n",
    "        images_dir=test_images_dir,\n",
    "        masks_dir=test_masks_dir,\n",
    "        file_list=all_test_images,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create DataLoaders\n",
    "    # ------------------------------\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,  # Adjust based on your CPU\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Initialize Model, Loss, Optimizer, Scheduler\n",
    "    # ------------------------------\n",
    "    model = ResNetUNet(n_classes=2, encoder_name='resnet34', pretrained=True).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    # Initialize Metrics\n",
    "    metrics_val = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    metrics_test = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device, epoch, metrics_val)\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\">> Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "            early_stopping.counter = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"[Main] Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n[Main] Training complete. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"[Main] Best model saved at: {save_path}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Testing\n",
    "    # ------------------------------\n",
    "    print(\"\\n>>> Loading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_metrics = test_segmentation(model, test_loader, device, metrics=metrics_test)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Visualization of Test Samples\n",
    "    # ------------------------------\n",
    "    if len(test_dataset) > 0:\n",
    "        print(\"\\n>>> Visualizing predictions on test samples...\")\n",
    "        visualize_predictions(model, test_dataset, device, num_samples=10, post_process_flag=post_process_flag)\n",
    "    else:\n",
    "        print(\"[Warning] No test samples available for visualization.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Run the Main Function\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models  # For ResNet encoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchmetrics\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# 0. Reproducibility (Optional)\n",
    "# ------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Ensures that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Dataset and Transforms\n",
    "# ------------------------------\n",
    "\n",
    "class ResizeAndToTensor:\n",
    "    \"\"\"\n",
    "    Custom transform to resize images and masks to a fixed size and convert them to tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(512, 512), augment=False):\n",
    "        self.size = size  # (height, width)\n",
    "        self.augment = augment\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL Image): Input image.\n",
    "            mask (np.array or PIL Image): Corresponding mask as a NumPy array or PIL Image.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor (torch.Tensor): Resized and normalized image tensor.\n",
    "            mask_tensor (torch.Tensor): Resized mask tensor (Long type).\n",
    "        \"\"\"\n",
    "        # Resize image using bilinear interpolation\n",
    "        image = resize(\n",
    "            image,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.BILINEAR\n",
    "        )\n",
    "        # Resize mask using nearest-neighbor interpolation to preserve label integrity\n",
    "        if isinstance(mask, np.ndarray):\n",
    "            mask_pil = Image.fromarray(mask)\n",
    "        elif isinstance(mask, Image.Image):\n",
    "            mask_pil = mask\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported mask type: {type(mask)}\")\n",
    "        \n",
    "        mask_pil = resize(\n",
    "            mask_pil,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.NEAREST\n",
    "        )\n",
    "        # Convert back to NumPy array\n",
    "        mask = np.array(mask_pil, dtype=np.uint8)\n",
    "\n",
    "        # Data Augmentation (if enabled)\n",
    "        if self.augment:\n",
    "            # Random horizontal flip\n",
    "            if random.random() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                mask = Image.fromarray(mask).transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "            # Random vertical flip\n",
    "            if random.random() > 0.5:\n",
    "                image = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                mask = Image.fromarray(mask).transpose(Image.FLIP_TOP_BOTTOM)\n",
    "                mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "            # Random rotation by 0°, 90°, 180°, or 270°\n",
    "            angles = [0, 90, 180, 270]\n",
    "            angle = random.choice(angles)\n",
    "            if angle != 0:\n",
    "                image = image.rotate(angle)\n",
    "                mask = Image.fromarray(mask).rotate(angle)\n",
    "                mask = np.array(mask, dtype=np.uint8)\n",
    "\n",
    "            # Add more augmentations as needed\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image_tensor = to_tensor(image)             # [C, H, W], float32 in [0,1]\n",
    "        image_tensor = self.normalize(image_tensor)\n",
    "\n",
    "        mask_tensor  = torch.from_numpy(mask).long()# [H, W], dtype=torch.int64\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "class BinarySegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for binary image segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, file_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with input images.\n",
    "            masks_dir (str): Directory with corresponding mask files (.npy or image files).\n",
    "            file_list (list): List of image filenames.\n",
    "            transform (callable, optional): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir  = masks_dir\n",
    "        self.file_list  = file_list\n",
    "        self.transform  = transform\n",
    "\n",
    "        # Debug: Print how many files in this split\n",
    "        print(f\"[Dataset] Initialized with {len(file_list)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask pair at the specified index.\n",
    "        \"\"\"\n",
    "        img_filename = self.file_list[idx]\n",
    "        base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"[Error] Image file not found: {img_path}\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        # Attempt to find mask with the same base name and supported extensions\n",
    "        mask_extensions = ['.npy', '.png', '.jpg', '.jpeg']\n",
    "        mask_path = None\n",
    "        for ext in mask_extensions:\n",
    "            potential_path = os.path.join(self.masks_dir, base_name + ext)\n",
    "            if os.path.isfile(potential_path):\n",
    "                mask_path = potential_path\n",
    "                break\n",
    "        if mask_path is None:\n",
    "            raise FileNotFoundError(f\"[Error] Mask file not found for image: {img_filename}\")\n",
    "\n",
    "        # Load mask\n",
    "        if mask_path.endswith('.npy'):\n",
    "            mask = np.load(mask_path)\n",
    "            mask = Image.fromarray(mask)\n",
    "        else:\n",
    "            mask = Image.open(mask_path).convert(\"L\")  # Convert to grayscale\n",
    "\n",
    "        # Remap any label >1 to 1 to ensure binary segmentation\n",
    "        mask_np = np.array(mask)\n",
    "        mask_np = np.where(mask_np > 1, 1, mask_np).astype(np.uint8)\n",
    "        mask = Image.fromarray(mask_np)\n",
    "\n",
    "        # Apply transformations if any\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Transformer Module Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512*512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, N, D]\n",
    "        return x + self.position_embeddings[:, :x.size(1), :]\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, embed_dim=512, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super(TransformerBottleneck, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(d_model=embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout, \n",
    "            dim_feedforward=embed_dim*4\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, H, W]\n",
    "        B, C, H, W = x.size()\n",
    "        N = H * W\n",
    "        # Flatten spatial dimensions\n",
    "        x = x.view(B, C, N).permute(0, 2, 1)  # [B, N, C]\n",
    "        x = self.positional_encoding(x)        # [B, N, C]\n",
    "        x = self.transformer_encoder(x)        # [B, N, C]\n",
    "        x = self.layer_norm(x)                 # [B, N, C]\n",
    "        # Reshape back to [B, C, H, W]\n",
    "        x = x.permute(0, 2, 1).view(B, C, H, W)  # [B, C, H, W]\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# 3. U-Net Model Definition with Transformer\n",
    "# ------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two convolutional layers each followed by BatchNorm and ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResNetUNetWithTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture with ResNet encoder and Transformer bottleneck.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=2, encoder_name='resnet34', pretrained=True, transformer_layers=6, transformer_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            encoder_name (str): Name of the ResNet encoder to use ('resnet18', 'resnet34', 'resnet50', etc.).\n",
    "            pretrained (bool): Whether to use pretrained ResNet weights.\n",
    "            transformer_layers (int): Number of Transformer encoder layers.\n",
    "            transformer_heads (int): Number of attention heads in Transformer.\n",
    "        \"\"\"\n",
    "        super(ResNetUNetWithTransformer, self).__init__()\n",
    "        \n",
    "        # Initialize ResNet encoder\n",
    "        if encoder_name == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet34':\n",
    "            self.encoder = models.resnet34(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet101':\n",
    "            self.encoder = models.resnet101(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet variant\")\n",
    "\n",
    "        # Encoder layers\n",
    "        self.initial = nn.Sequential(\n",
    "            self.encoder.conv1,  # [B, 64, H/2, W/2]\n",
    "            self.encoder.bn1,\n",
    "            self.encoder.relu,\n",
    "            self.encoder.maxpool  # [B, 64, H/4, W/4]\n",
    "        )\n",
    "        self.encoder_layer1 = self.encoder.layer1  # [B, 64 or 256, H/4, W/4]\n",
    "        self.encoder_layer2 = self.encoder.layer2  # [B, 128 or 512, H/8, W/8]\n",
    "        self.encoder_layer3 = self.encoder.layer3  # [B, 256 or 1024, H/16, W/16]\n",
    "        self.encoder_layer4 = self.encoder.layer4  # [B, 512 or 2048, H/32, W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        self.transformer = TransformerBottleneck(\n",
    "            embed_dim=encoder_channels[4],\n",
    "            num_heads=transformer_heads,\n",
    "            num_layers=transformer_layers,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        # Corrected up4 to take encoder_channels[4] as input and output encoder_channels[3]\n",
    "        self.up4 = nn.ConvTranspose2d(encoder_channels[4], encoder_channels[3], kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(encoder_channels[3] + encoder_channels[3], encoder_channels[3])\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(encoder_channels[3], encoder_channels[2], kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(encoder_channels[2] + encoder_channels[2], encoder_channels[2])\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(encoder_channels[2], encoder_channels[1], kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(encoder_channels[1] + encoder_channels[1], encoder_channels[1])\n",
    "\n",
    "        \n",
    "\n",
    "        self.out_conv = nn.Conv2d(encoder_channels[1], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.initial(x)           # [B, 64, H/4, W/4]\n",
    "        x1 = self.encoder_layer1(x0)   # [B, 64, H/4, W/4] for resnet34\n",
    "        x2 = self.encoder_layer2(x1)   # [B, 128, H/8, W/8]\n",
    "        x3 = self.encoder_layer3(x2)   # [B, 256, H/16, W/16]\n",
    "        x4 = self.encoder_layer4(x3)   # [B, 512, H/32, W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        x4 = self.transformer(x4)      # [B, 512, H/32, W/32]\n",
    "\n",
    "        # Decoder\n",
    "        up4 = self.up4(x4)             # [B, 256, H/16, W/16]\n",
    "        merge4 = torch.cat([up4, x3], dim=1)  # [B, 512, H/16, W/16]\n",
    "        conv4 = self.conv4(merge4)     # [B, 256, H/16, W/16]\n",
    "\n",
    "        up3 = self.up3(conv4)          # [B, 128, H/8, W/8]\n",
    "        merge3 = torch.cat([up3, x2], dim=1)  # [B, 256, H/8, W/8]\n",
    "        conv3 = self.conv3(merge3)     # [B, 128, H/8, W/8]\n",
    "\n",
    "        up2 = self.up2(conv3)          # [B, 64, H/4, W/4]\n",
    "        merge2 = torch.cat([up2, x1], dim=1)  # [B, 128, H/4, W/4]\n",
    "        conv2 = self.conv2(merge2)     # [B, 64, H/4, W/4]\n",
    "\n",
    "\n",
    "        out = self.out_conv(conv2)     # [B, n_classes, H/2, W/2]\n",
    "\n",
    "        # Upsample to original size\n",
    "        out = nn.functional.interpolate(out, scale_factor=4, mode='bilinear', align_corners=True)  # [B, n_classes, H, W]\n",
    "\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Early Stopping Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Initial best loss: {self.best_loss:.4f}\")\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"[EarlyStopping] Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Validation loss improved to: {self.best_loss:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Training and Validation Functions\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch_idx):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Train] Starting epoch {epoch_idx+1} ---\")\n",
    "    # Use tqdm to show progress\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Train] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Train] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    return average_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, epoch_idx, metrics=None):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics to compute.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Val] Starting epoch {epoch_idx+1} ---\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Val] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Val] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    \n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        metric_values = {}\n",
    "    \n",
    "    # Optionally, print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Val] Epoch {epoch_idx+1} Metrics: {metric_str} ---\")\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Testing and Metrics\n",
    "# ------------------------------\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient for binary masks.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted mask [H, W].\n",
    "        target (torch.Tensor): Ground truth mask [H, W].\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1).float()\n",
    "    target_flat = target.view(-1).float()\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def test_segmentation(model, loader, device, metrics=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and computes the average Dice coefficient and other metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): Device to run on.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of average metrics on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Testing\", leave=False)):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)  # [B, H, W]\n",
    "\n",
    "            outputs = model(images)    # [B, n_classes, H, W]\n",
    "            preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        # If metrics are not provided, compute only Dice\n",
    "        dice_scores = []\n",
    "        for pred, mask in zip(preds, masks):\n",
    "            dice = dice_coefficient(pred, mask)\n",
    "            dice_scores.append(dice)\n",
    "        metric_values = {\n",
    "            'dice': np.mean(dice_scores) if dice_scores else 0\n",
    "        }\n",
    "\n",
    "    # Print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Test] Metrics: {metric_str} ---\")\n",
    "    else:\n",
    "        print(f\"--- [Test] Dice Coefficient: {metric_values['dice']:.4f} ---\")\n",
    "\n",
    "    return metric_values\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Visualization of Predictions\n",
    "# ------------------------------\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=3, post_process_flag=False):\n",
    "    \"\"\"\n",
    "    Visualizes predictions alongside the original images and ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        dataset (Dataset): The test dataset.\n",
    "        device (torch.device): Device to run on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        post_process_flag (bool): Whether to apply post-processing to predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_batch = image.unsqueeze(0).to(device)  # [1, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image_batch)  # [1, n_classes, H, W]\n",
    "            pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "\n",
    "        # No post-processing as cv2 is removed\n",
    "        # If needed, you can implement torch-based post-processing here\n",
    "\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "        mask_np = mask.cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Handle image normalization for visualization\n",
    "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "        image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(mask_np, cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(pred, cmap='gray')\n",
    "        axs[2].set_title(\"Predicted Mask\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Main Function\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # Define Your Directories Here\n",
    "    # ------------------------------\n",
    "    # Training and Validation directories (split from the same folder)\n",
    "    images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_train/images/\"\n",
    "    masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_train/masks/\"\n",
    "\n",
    "    # Testing directories (completely separate)\n",
    "    test_images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_test/images/\"\n",
    "    test_masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_test/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size    = 4\n",
    "    num_epochs    = 20\n",
    "    learning_rate = 1e-3\n",
    "    val_split     = 0.2\n",
    "    save_path     = \"best_model.pth\"\n",
    "    patience      = 7\n",
    "    post_process_flag = False  # Set to True to apply post-processing if implemented\n",
    "\n",
    "    # ------------------------------\n",
    "    # Device Configuration\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Main] Using device: {device}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect Training File Lists\n",
    "    # ------------------------------\n",
    "    # List all training image files\n",
    "    all_train_images = sorted([\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_images)} training image files in {images_dir}\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training image files found. Check your training path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding mask files exist\n",
    "    all_train_images = sorted([\n",
    "        f for f in all_train_images\n",
    "        if any(\n",
    "            os.path.isfile(os.path.join(masks_dir, os.path.splitext(f)[0] + ext))\n",
    "            for ext in ['.npy', '.png', '.jpg', '.jpeg']\n",
    "        )\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_train_images)} training images have corresponding masks.\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training mask files found or mismatched filenames. Check your training mask path!\")\n",
    "        return\n",
    "\n",
    "    # List all test image files\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(test_images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_images)} test image files in {test_images_dir}\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test image files found. Check your test path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding test mask files exist\n",
    "    all_test_images = sorted([\n",
    "        f for f in all_test_images\n",
    "        if any(\n",
    "            os.path.isfile(os.path.join(test_masks_dir, os.path.splitext(f)[0] + ext))\n",
    "            for ext in ['.npy', '.png', '.jpg', '.jpeg']\n",
    "        )\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_test_images)} test images have corresponding masks.\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test mask files found or mismatched filenames. Check your test mask path!\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------\n",
    "    # Train/Validation Split\n",
    "    # ------------------------------\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_train_images,\n",
    "        test_size=val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"[Main] Training samples: {len(train_files)}\")\n",
    "    print(f\"[Main] Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Datasets with Transforms\n",
    "    # ------------------------------\n",
    "    train_transform = ResizeAndToTensor(size=(512, 512), augment=True)\n",
    "    val_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "    test_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "\n",
    "    train_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=train_files,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=val_files,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = BinarySegDataset(\n",
    "        images_dir=test_images_dir,\n",
    "        masks_dir=test_masks_dir,\n",
    "        file_list=all_test_images,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create DataLoaders\n",
    "    # ------------------------------\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=4,  # Adjust based on your CPU cores\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Initialize Model, Loss, Optimizer, Scheduler\n",
    "    # ------------------------------\n",
    "    model = ResNetUNetWithTransformer(\n",
    "        n_classes=2, \n",
    "        encoder_name='resnet50', \n",
    "        pretrained=True, \n",
    "        transformer_layers=8, \n",
    "        transformer_heads=16\n",
    "    ).to(device)\n",
    "    \n",
    "    # Cross-Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    # Initialize Metrics\n",
    "    metrics_val = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    metrics_test = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device, epoch, metrics_val)\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\">> Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "            early_stopping.counter = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"[Main] Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n[Main] Training complete. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"[Main] Best model saved at: {save_path}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Testing\n",
    "    # ------------------------------\n",
    "    print(\"\\n>>> Loading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_metrics = test_segmentation(model, test_loader, device, metrics=metrics_test)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Visualization of Test Samples\n",
    "    # ------------------------------\n",
    "    if len(test_dataset) > 0:\n",
    "        print(\"\\n>>> Visualizing predictions on test samples...\")\n",
    "        visualize_predictions(model, test_dataset, device, num_samples=3, post_process_flag=post_process_flag)\n",
    "    else:\n",
    "        print(\"[Warning] No test samples available for visualization.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Run the Main Function\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models  # For ResNet encoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import torchmetrics\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# 0. Reproducibility (Optional)\n",
    "# ------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Ensures that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Dataset and Transforms\n",
    "# ------------------------------\n",
    "\n",
    "class ResizeAndToTensor:\n",
    "    \"\"\"\n",
    "    Custom transform to resize images and masks to a fixed size and convert them to tensors.\n",
    "    Removes data augmentations like flipping and rotating.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(512, 512)):\n",
    "        self.size = size  # (height, width)\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL Image): Input image.\n",
    "            mask (np.array): Corresponding mask as a NumPy array.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor (torch.Tensor): Resized and normalized image tensor.\n",
    "            mask_tensor (torch.Tensor): Resized mask tensor (Long type).\n",
    "        \"\"\"\n",
    "        # Resize image using bilinear interpolation\n",
    "        image = resize(\n",
    "            image,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.BILINEAR\n",
    "        )\n",
    "        # Resize mask using nearest-neighbor interpolation to preserve label integrity\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        mask_pil = resize(\n",
    "            mask_pil,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.NEAREST\n",
    "        )\n",
    "        # Convert back to NumPy array\n",
    "        mask = np.array(mask_pil, dtype=np.uint8)\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image_tensor = to_tensor(image)             # [C, H, W], float32 in [0,1]\n",
    "        # Normalize the image tensor\n",
    "        image_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                            std=[0.229, 0.224, 0.225])(image_tensor)\n",
    "\n",
    "        mask_tensor  = torch.from_numpy(mask).long()# [H, W], dtype=torch.int64\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "class BinarySegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for binary image segmentation.\n",
    "    Assumes that mask files are stored as .npy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, file_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with input images.\n",
    "            masks_dir (str): Directory with corresponding mask files (.npy).\n",
    "            file_list (list): List of image filenames.\n",
    "            transform (callable, optional): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir  = masks_dir\n",
    "        self.file_list  = file_list\n",
    "        self.transform  = transform\n",
    "\n",
    "        # Debug: Print how many files in this split\n",
    "        print(f\"[Dataset] Initialized with {len(file_list)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask pair at the specified index.\n",
    "        \"\"\"\n",
    "        img_filename = self.file_list[idx]\n",
    "        base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"[Error] Image file not found: {img_path}\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.masks_dir, base_name + \".npy\")\n",
    "        if not os.path.isfile(mask_path):\n",
    "            raise FileNotFoundError(f\"[Error] Mask file not found: {mask_path}\")\n",
    "        mask = np.load(mask_path)\n",
    "\n",
    "        # Remap label >1 to 1 to ensure binary segmentation\n",
    "        mask = np.where(mask > 1, 1, mask).astype(np.uint8)\n",
    "\n",
    "        # Apply transform\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Transformer Module Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512*512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, N, D]\n",
    "        return x + self.position_embeddings[:, :x.size(1), :]\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, embed_dim=2048, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super(TransformerBottleneck, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(d_model=embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout, \n",
    "            dim_feedforward=embed_dim*4\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, H, W]\n",
    "        B, C, H, W = x.size()\n",
    "        N = H * W\n",
    "        # Flatten spatial dimensions\n",
    "        x = x.view(B, C, N).permute(0, 2, 1)  # [B, N, C]\n",
    "        x = self.positional_encoding(x)        # [B, N, C]\n",
    "        x = self.transformer_encoder(x)        # [B, N, C]\n",
    "        x = self.layer_norm(x)                 # [B, N, C]\n",
    "        # Reshape back to [B, C, H, W]\n",
    "        x = x.permute(0, 2, 1).view(B, C, H, W)  # [B, C, H, W]\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# 3. U-Net Model Definition with Transformer\n",
    "# ------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two convolutional layers each followed by BatchNorm and ReLU.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class ResNetUNetWithTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture with ResNet encoder and Transformer bottleneck.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=2, encoder_name='resnet101', pretrained=True, transformer_layers=6, transformer_heads=8):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            encoder_name (str): Name of the ResNet encoder to use ('resnet18', 'resnet34', 'resnet50', etc.).\n",
    "            pretrained (bool): Whether to use pretrained ResNet weights.\n",
    "            transformer_layers (int): Number of Transformer encoder layers.\n",
    "            transformer_heads (int): Number of attention heads in Transformer.\n",
    "        \"\"\"\n",
    "        super(ResNetUNetWithTransformer, self).__init__()\n",
    "        \n",
    "        # Initialize ResNet encoder\n",
    "        if encoder_name == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet34':\n",
    "            self.encoder = models.resnet34(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet101':\n",
    "            self.encoder = models.resnet101(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet152':\n",
    "            self.encoder = models.resnet152(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet variant\")\n",
    "\n",
    "        # Encoder layers\n",
    "        self.initial = nn.Sequential(\n",
    "            self.encoder.conv1,  # [B, 64, H/2, W/2]\n",
    "            self.encoder.bn1,\n",
    "            self.encoder.relu,\n",
    "            self.encoder.maxpool  # [B, 64, H/4, W/4]\n",
    "        )\n",
    "        self.encoder_layer1 = self.encoder.layer1  # [B, 256, H/4, W/4] for ResNet101\n",
    "        self.encoder_layer2 = self.encoder.layer2  # [B, 512, H/8, W/8]\n",
    "        self.encoder_layer3 = self.encoder.layer3  # [B, 1024, H/16, W/16]\n",
    "        self.encoder_layer4 = self.encoder.layer4  # [B, 2048, H/32, W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        self.transformer = TransformerBottleneck(\n",
    "            embed_dim=encoder_channels[4],\n",
    "            num_heads=transformer_heads,\n",
    "            num_layers=transformer_layers,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        # up4: [B, 2048, H/32, W/32] -> [B,1024,H/16,W/16]\n",
    "        self.up4 = nn.ConvTranspose2d(encoder_channels[4], encoder_channels[3], kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(encoder_channels[3] + encoder_channels[3], encoder_channels[3])\n",
    "\n",
    "        # up3: [B,1024,H/16,W/16] -> [B,512,H/8,W/8]\n",
    "        self.up3 = nn.ConvTranspose2d(encoder_channels[3], encoder_channels[2], kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(encoder_channels[2] + encoder_channels[2], encoder_channels[2])\n",
    "\n",
    "        # up2: [B,512,H/8,W/8] -> [B,256,H/4,W/4]\n",
    "        self.up2 = nn.ConvTranspose2d(encoder_channels[2], encoder_channels[1], kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(encoder_channels[1] + encoder_channels[1], encoder_channels[1])\n",
    "\n",
    "        # up1: [B,256,H/4,W/4] -> [B,64,H/4,W/4] (no upsampling)\n",
    "        self.up1 = nn.ConvTranspose2d(encoder_channels[1], encoder_channels[0], kernel_size=1, stride=1)\n",
    "        self.conv1 = DoubleConv(encoder_channels[0] + encoder_channels[0], encoder_channels[0])\n",
    "\n",
    "        # Output convolution\n",
    "        self.out_conv = nn.Conv2d(encoder_channels[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.initial(x)           # [B,64,H/4,W/4]\n",
    "        x1 = self.encoder_layer1(x0)   # [B,256,H/4,W/4] for ResNet101\n",
    "        x2 = self.encoder_layer2(x1)   # [B,512,H/8,W/8]\n",
    "        x3 = self.encoder_layer3(x2)   # [B,1024,H/16,W/16]\n",
    "        x4 = self.encoder_layer4(x3)   # [B,2048,H/32,W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        x4 = self.transformer(x4)      # [B,2048,H/32,W/32]\n",
    "\n",
    "        # Decoder\n",
    "        up4 = self.up4(x4)             # [B,1024,H/16,W/16]\n",
    "        merge4 = torch.cat([up4, x3], dim=1)  # [B,2048,H/16,W/16]\n",
    "        conv4 = self.conv4(merge4)     # [B,1024,H/16,W/16]\n",
    "\n",
    "        up3 = self.up3(conv4)          # [B,512,H/8,W/8]\n",
    "        merge3 = torch.cat([up3, x2], dim=1)  # [B,1024,H/8,W/8]\n",
    "        conv3 = self.conv3(merge3)     # [B,512,H/8,W/8]\n",
    "\n",
    "        up2 = self.up2(conv3)          # [B,256,H/4,W/4]\n",
    "        merge2 = torch.cat([up2, x1], dim=1)  # [B,512,H/4,W/4]\n",
    "        conv2 = self.conv2(merge2)     # [B,256,H/4,W/4]\n",
    "\n",
    "        up1 = self.up1(conv2)          # [B,64,H/4,W/4]\n",
    "        merge1 = torch.cat([up1, x0], dim=1)  # [B,128,H/4,W/4]\n",
    "        conv1 = self.conv1(merge1)     # [B,64,H/4,W/4]\n",
    "\n",
    "        # Output convolution\n",
    "        out = self.out_conv(conv1)     # [B,2,H/4,W/4]\n",
    "\n",
    "        # Upsample to original size\n",
    "        out = nn.functional.interpolate(out, scale_factor=4, mode='bilinear', align_corners=True)  # [B,2,H,W]\n",
    "\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Early Stopping Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Initial best loss: {self.best_loss:.4f}\")\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"[EarlyStopping] Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Validation loss improved to: {self.best_loss:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Training and Validation Functions\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch_idx, scaler):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using Mixed Precision.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        scaler (torch.cuda.amp.GradScaler): Gradient scaler for mixed precision.\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Train] Starting epoch {epoch_idx+1} ---\")\n",
    "    # Use tqdm to show progress\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Train] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Train] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    return average_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, epoch_idx, metrics=None):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch using Mixed Precision.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics to compute.\n",
    "\n",
    "    Returns:\n",
    "        float: Average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Val] Starting epoch {epoch_idx+1} ---\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Val] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Val] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    \n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        metric_values = {}\n",
    "    \n",
    "    # Optionally, print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Val] Epoch {epoch_idx+1} Metrics: {metric_str} ---\")\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Testing and Metrics\n",
    "# ------------------------------\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient for binary masks.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted mask [H, W].\n",
    "        target (torch.Tensor): Ground truth mask [H, W].\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1).float()\n",
    "    target_flat = target.view(-1).float()\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def test_segmentation(model, loader, device, metrics=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and computes the average Dice coefficient and other metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): Device to run on.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of average metrics on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Testing\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)  # [B, H, W]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)    # [B, n_classes, H, W]\n",
    "                preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        # If metrics are not provided, compute only Dice\n",
    "        dice_scores = []\n",
    "        for pred, mask in zip(preds, masks):\n",
    "            dice = dice_coefficient(pred, mask)\n",
    "            dice_scores.append(dice)\n",
    "        metric_values = {\n",
    "            'dice': np.mean(dice_scores) if dice_scores else 0\n",
    "        }\n",
    "\n",
    "    # Print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Test] Metrics: {metric_str} ---\")\n",
    "    else:\n",
    "        print(f\"--- [Test] Dice Coefficient: {metric_values['dice']:.4f} ---\")\n",
    "\n",
    "    return metric_values\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Visualization of Predictions\n",
    "# ------------------------------\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=3, post_process_flag=False):\n",
    "    \"\"\"\n",
    "    Visualizes predictions alongside the original images and ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        dataset (Dataset): The test dataset.\n",
    "        device (torch.device): Device to run on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        post_process_flag (bool): Whether to apply post-processing to predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_batch = image.unsqueeze(0).to(device, non_blocking=True)  # [1, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(image_batch)  # [1, n_classes, H, W]\n",
    "                pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "\n",
    "        # No post-processing as cv2 is removed\n",
    "        # If needed, you can implement torch-based post-processing here\n",
    "\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "        mask_np = mask.cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Handle image normalization for visualization\n",
    "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "        image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(mask_np, cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(pred, cmap='gray')\n",
    "        axs[2].set_title(\"Predicted Mask\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Main Function\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # Define Your Directories Here\n",
    "    # ------------------------------\n",
    "    # Training and Validation directories (split from the same folder)\n",
    "    images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_train/images/\"\n",
    "    masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_train/masks/\"\n",
    "\n",
    "    # Testing directories (completely separate)\n",
    "    test_images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_test/images/\"\n",
    "    test_masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_test/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size    = 8  # Increased batch size for speed; adjust based on GPU memory\n",
    "    num_epochs    = 30  # Increased epochs to allow convergence with ResNet101\n",
    "    learning_rate = 1e-4\n",
    "    val_split     = 0.2\n",
    "    save_path     = \"best_model_resnet101.pth\"\n",
    "    patience      = 10\n",
    "    post_process_flag = False  # Set to True to apply post-processing if implemented\n",
    "\n",
    "    # ------------------------------\n",
    "    # Device Configuration\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Main] Using device: {device}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect Training File Lists\n",
    "    # ------------------------------\n",
    "    # List all training image files\n",
    "    all_train_images = sorted([\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_images)} training image files in {images_dir}\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training image files found. Check your training path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding mask files exist\n",
    "    all_train_images = sorted([\n",
    "        f for f in all_train_images\n",
    "        if os.path.isfile(os.path.join(masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_train_images)} training images have corresponding masks.\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training mask files found or mismatched filenames. Check your training mask path!\")\n",
    "        return\n",
    "\n",
    "    # List all test image files\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(test_images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_images)} test image files in {test_images_dir}\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test image files found. Check your test path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding test mask files exist\n",
    "    all_test_images = sorted([\n",
    "        f for f in all_test_images\n",
    "        if os.path.isfile(os.path.join(test_masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_test_images)} test images have corresponding masks.\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test mask files found or mismatched filenames. Check your test mask path!\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------\n",
    "    # Train/Validation Split\n",
    "    # ------------------------------\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_train_images,\n",
    "        test_size=val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"[Main] Training samples: {len(train_files)}\")\n",
    "    print(f\"[Main] Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Datasets with Transforms\n",
    "    # ------------------------------\n",
    "    train_transform = ResizeAndToTensor(size=(512, 512))\n",
    "    val_transform = ResizeAndToTensor(size=(512, 512))\n",
    "    test_transform = ResizeAndToTensor(size=(512, 512))\n",
    "\n",
    "    train_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=train_files,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=val_files,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = BinarySegDataset(\n",
    "        images_dir=test_images_dir,\n",
    "        masks_dir=test_masks_dir,\n",
    "        file_list=all_test_images,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create DataLoaders\n",
    "    # ------------------------------\n",
    "    num_workers = 8  # Increased number of workers for faster data loading; adjust based on CPU cores\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Initialize Model, Loss, Optimizer, Scheduler\n",
    "    # ------------------------------\n",
    "    model = ResNetUNetWithTransformer(\n",
    "        n_classes=2, \n",
    "        encoder_name='resnet101', \n",
    "        pretrained=True, \n",
    "        transformer_layers=6, \n",
    "        transformer_heads=8\n",
    "    ).to(device)\n",
    "    \n",
    "    # Cross-Entropy Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    # Initialize Metrics\n",
    "    metrics_val = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    metrics_test = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    # Initialize Gradient Scaler for Mixed Precision\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, scaler)\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device, epoch, metrics_val)\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(epoch + val_split)  # Using epoch count including validation split\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\">> Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "            early_stopping.counter = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"[Main] Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n[Main] Training complete. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"[Main] Best model saved at: {save_path}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Testing\n",
    "    # ------------------------------\n",
    "    print(\"\\n>>> Loading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_metrics = test_segmentation(model, test_loader, device, metrics=metrics_test)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Visualization of Test Samples\n",
    "    # ------------------------------\n",
    "    if len(test_dataset) > 0:\n",
    "        print(\"\\n>>> Visualizing predictions on test samples...\")\n",
    "        visualize_predictions(model, test_dataset, device, num_samples=3, post_process_flag=post_process_flag)\n",
    "    else:\n",
    "        print(\"[Warning] No test samples available for visualization.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Run the Main Function\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models  # For ResNet encoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import torchmetrics\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# 0. Reproducibility (Optional)\n",
    "# ------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Ensures that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Dataset and Transforms\n",
    "# ------------------------------\n",
    "\n",
    "class ResizeAndToTensor:\n",
    "    \"\"\"\n",
    "    Custom transform to resize images and masks to a fixed size and convert them to tensors.\n",
    "    Removes data augmentations like flipping and rotating.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(512, 512)):\n",
    "        self.size = size  # (height, width)\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL Image): Input image.\n",
    "            mask (np.array): Corresponding mask as a NumPy array.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor (torch.Tensor): Resized and normalized image tensor.\n",
    "            mask_tensor (torch.Tensor): Resized mask tensor (Long type).\n",
    "        \"\"\"\n",
    "        # Resize image using bilinear interpolation\n",
    "        image = resize(\n",
    "            image,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.BILINEAR\n",
    "        )\n",
    "        # Resize mask using nearest-neighbor interpolation to preserve label integrity\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        mask_pil = resize(\n",
    "            mask_pil,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.NEAREST\n",
    "        )\n",
    "        # Convert back to NumPy array\n",
    "        mask = np.array(mask_pil, dtype=np.uint8)\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image_tensor = to_tensor(image)             # [C, H, W], float32 in [0,1]\n",
    "        image_tensor = self.normalize(image_tensor)\n",
    "        mask_tensor  = torch.from_numpy(mask).long()# [H, W], dtype=torch.int64\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "class BinarySegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for binary image segmentation.\n",
    "    Assumes that mask files are stored as .npy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, file_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with input images.\n",
    "            masks_dir (str): Directory with corresponding mask files (.npy).\n",
    "            file_list (list): List of image filenames.\n",
    "            transform (callable, optional): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir  = masks_dir\n",
    "        self.file_list  = file_list\n",
    "        self.transform  = transform\n",
    "\n",
    "        # Debug: Print how many files in this split\n",
    "        print(f\"[Dataset] Initialized with {len(file_list)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask pair at the specified index.\n",
    "        \"\"\"\n",
    "        img_filename = self.file_list[idx]\n",
    "        base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"[Error] Image file not found: {img_path}\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.masks_dir, base_name + \".npy\")\n",
    "        if not os.path.isfile(mask_path):\n",
    "            raise FileNotFoundError(f\"[Error] Mask file not found: {mask_path}\")\n",
    "        mask = np.load(mask_path)\n",
    "\n",
    "        # Remap label >1 to 1 to ensure binary segmentation\n",
    "        mask = np.where(mask > 1, 1, mask).astype(np.uint8)\n",
    "\n",
    "        # Apply transform\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Transformer Module Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512*512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, N, D]\n",
    "        return x + self.position_embeddings[:, :x.size(1), :]\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, embed_dim=2048, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super(TransformerBottleneck, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(d_model=embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, \n",
    "            nhead=num_heads, \n",
    "            dropout=dropout, \n",
    "            dim_feedforward=embed_dim*4\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, H, W]\n",
    "        B, C, H, W = x.size()\n",
    "        N = H * W\n",
    "        # Flatten spatial dimensions\n",
    "        x = x.view(B, C, N).permute(0, 2, 1)  # [B, N, C]\n",
    "        x = self.positional_encoding(x)        # [B, N, C]\n",
    "        x = self.transformer_encoder(x)        # [B, N, C]\n",
    "        x = self.layer_norm(x)                 # [B, N, C]\n",
    "        # Reshape back to [B, C, H, W]\n",
    "        x = x.permute(0, 2, 1).view(B, C, H, W)  # [B, C, H, W]\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# 3. U-Net Model Definition with Transformer\n",
    "# ------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two convolutional layers each followed by BatchNorm, ReLU, and Dropout.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_p=0.5):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for binary segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, logits, true):\n",
    "        probs = nn.functional.softmax(logits, dim=1)\n",
    "        true_one_hot = nn.functional.one_hot(true, num_classes=probs.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(probs * true_one_hot, dims)\n",
    "        cardinality = torch.sum(probs + true_one_hot, dims)\n",
    "        dice = (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class ResNetUNetWithTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture with ResNet encoder and Transformer bottleneck.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=2, encoder_name='resnet50', pretrained=True, transformer_layers=6, transformer_heads=8, dropout_p=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            encoder_name (str): Name of the ResNet encoder to use ('resnet18', 'resnet34', 'resnet50', etc.).\n",
    "            pretrained (bool): Whether to use pretrained ResNet weights.\n",
    "            transformer_layers (int): Number of Transformer encoder layers.\n",
    "            transformer_heads (int): Number of attention heads in Transformer.\n",
    "            dropout_p (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(ResNetUNetWithTransformer, self).__init__()\n",
    "        \n",
    "        # Initialize ResNet encoder\n",
    "        if encoder_name == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet34':\n",
    "            self.encoder = models.resnet34(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet101':\n",
    "            self.encoder = models.resnet101(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet152':\n",
    "            self.encoder = models.resnet152(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet variant\")\n",
    "\n",
    "        # Encoder layers\n",
    "        self.initial = nn.Sequential(\n",
    "            self.encoder.conv1,  # [B, 64, H/2, W/2]\n",
    "            self.encoder.bn1,\n",
    "            self.encoder.relu,\n",
    "            self.encoder.maxpool  # [B, 64, H/4, W/4]\n",
    "        )\n",
    "        self.encoder_layer1 = self.encoder.layer1  # [B, 256, H/4, W/4] for ResNet101\n",
    "        self.encoder_layer2 = self.encoder.layer2  # [B, 512, H/8, W/8]\n",
    "        self.encoder_layer3 = self.encoder.layer3  # [B, 1024, H/16, W/16]\n",
    "        self.encoder_layer4 = self.encoder.layer4  # [B, 2048, H/32, W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        self.transformer = TransformerBottleneck(\n",
    "            embed_dim=encoder_channels[4],\n",
    "            num_heads=transformer_heads,\n",
    "            num_layers=transformer_layers,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        # up4: [B, 2048, H/32, W/32] -> [B,1024,H/16,W/16]\n",
    "        self.up4 = nn.ConvTranspose2d(encoder_channels[4], encoder_channels[3], kernel_size=2, stride=2)\n",
    "        self.conv4 = DoubleConv(encoder_channels[3] + encoder_channels[3], encoder_channels[3], dropout_p=dropout_p)\n",
    "\n",
    "        # up3: [B,1024,H/16,W/16] -> [B,512,H/8,W/8]\n",
    "        self.up3 = nn.ConvTranspose2d(encoder_channels[3], encoder_channels[2], kernel_size=2, stride=2)\n",
    "        self.conv3 = DoubleConv(encoder_channels[2] + encoder_channels[2], encoder_channels[2], dropout_p=dropout_p)\n",
    "\n",
    "        # up2: [B,512,H/8,W/8] -> [B,256,H/4,W/4]\n",
    "        self.up2 = nn.ConvTranspose2d(encoder_channels[2], encoder_channels[1], kernel_size=2, stride=2)\n",
    "        self.conv2 = DoubleConv(encoder_channels[1] + encoder_channels[1], encoder_channels[1], dropout_p=dropout_p)\n",
    "\n",
    "        # up1: [B,256,H/4,W/4] -> [B,64,H/4,W/4] (no upsampling)\n",
    "        self.up1 = nn.ConvTranspose2d(encoder_channels[1], encoder_channels[0], kernel_size=1, stride=1)\n",
    "        self.conv1 = DoubleConv(encoder_channels[0] + encoder_channels[0], encoder_channels[0], dropout_p=dropout_p)\n",
    "\n",
    "        # Output convolution\n",
    "        self.out_conv = nn.Conv2d(encoder_channels[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.initial(x)           # [B,64,H/4,W/4]\n",
    "        x1 = self.encoder_layer1(x0)   # [B,256,H/4,W/4] for ResNet101\n",
    "        x2 = self.encoder_layer2(x1)   # [B,512,H/8,W/8]\n",
    "        x3 = self.encoder_layer3(x2)   # [B,1024,H/16,W/16]\n",
    "        x4 = self.encoder_layer4(x3)   # [B,2048,H/32,W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        x4 = self.transformer(x4)      # [B,2048,H/32,W/32]\n",
    "\n",
    "        # Decoder\n",
    "        up4 = self.up4(x4)             # [B,1024,H/16,W/16]\n",
    "        merge4 = torch.cat([up4, x3], dim=1)  # [B,2048,H/16,W/16]\n",
    "        conv4 = self.conv4(merge4)     # [B,1024,H/16,W/16]\n",
    "\n",
    "        up3 = self.up3(conv4)          # [B,512,H/8,W/8]\n",
    "        merge3 = torch.cat([up3, x2], dim=1)  # [B,1024,H/8,W/8]\n",
    "        conv3 = self.conv3(merge3)     # [B,512,H/8,W/8]\n",
    "\n",
    "        up2 = self.up2(conv3)          # [B,256,H/4,W/4]\n",
    "        merge2 = torch.cat([up2, x1], dim=1)  # [B,512,H/4,W/4]\n",
    "        conv2 = self.conv2(merge2)     # [B,256,H/4,W/4]\n",
    "\n",
    "        up1 = self.up1(conv2)          # [B,64,H/4,W/4]\n",
    "        merge1 = torch.cat([up1, x0], dim=1)  # [B,128,H/4,W/4]\n",
    "        conv1 = self.conv1(merge1)     # [B,64,H/4,W/4]\n",
    "\n",
    "        # Output convolution\n",
    "        out = self.out_conv(conv1)     # [B,2,H/4,W/4]\n",
    "\n",
    "        # Upsample to original size\n",
    "        out = nn.functional.interpolate(out, scale_factor=4, mode='bilinear', align_corners=True)  # [B,2,H,W]\n",
    "\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Early Stopping Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Initial best loss: {self.best_loss:.4f}\")\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"[EarlyStopping] Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Validation loss improved to: {self.best_loss:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Training and Validation Functions\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch_idx, scaler):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using Mixed Precision.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        scaler (torch.cuda.amp.GradScaler): Gradient scaler for mixed precision.\n",
    "\n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Train] Starting epoch {epoch_idx+1} ---\")\n",
    "    # Use tqdm to show progress\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Train] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Train] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    return average_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, epoch_idx, metrics=None):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch using Mixed Precision.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (Loss): Loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics to compute.\n",
    "\n",
    "    Returns:\n",
    "        float: Average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Val] Starting epoch {epoch_idx+1} ---\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Val] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Val] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    \n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        metric_values = {}\n",
    "    \n",
    "    # Optionally, print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Val] Epoch {epoch_idx+1} Metrics: {metric_str} ---\")\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Testing and Metrics\n",
    "# ------------------------------\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient for binary masks.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted mask [H, W].\n",
    "        target (torch.Tensor): Ground truth mask [H, W].\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "\n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1).float()\n",
    "    target_flat = target.view(-1).float()\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def test_segmentation(model, loader, device, metrics=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and computes the average Dice coefficient and other metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): Device to run on.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of average metrics on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Testing\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)  # [B, H, W]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)    # [B, n_classes, H, W]\n",
    "                preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        # If metrics are not provided, compute only Dice\n",
    "        dice_scores = []\n",
    "        for pred, mask in zip(preds, masks):\n",
    "            dice = dice_coefficient(pred, mask)\n",
    "            dice_scores.append(dice)\n",
    "        metric_values = {\n",
    "            'dice': np.mean(dice_scores) if dice_scores else 0\n",
    "        }\n",
    "\n",
    "    # Print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Test] Metrics: {metric_str} ---\")\n",
    "    else:\n",
    "        print(f\"--- [Test] Dice Coefficient: {metric_values['dice']:.4f} ---\")\n",
    "\n",
    "    return metric_values\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Visualization of Predictions\n",
    "# ------------------------------\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=3, post_process_flag=False):\n",
    "    \"\"\"\n",
    "    Visualizes predictions alongside the original images and ground truth masks.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        dataset (Dataset): The test dataset.\n",
    "        device (torch.device): Device to run on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        post_process_flag (bool): Whether to apply post-processing to predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_batch = image.unsqueeze(0).to(device, non_blocking=True)  # [1, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(image_batch)  # [1, n_classes, H, W]\n",
    "                pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "\n",
    "        # No post-processing as cv2 is removed\n",
    "        # If needed, you can implement torch-based post-processing here\n",
    "\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "        mask_np = mask.cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Handle image normalization for visualization\n",
    "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "        image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(mask_np, cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(pred, cmap='gray')\n",
    "        axs[2].set_title(\"Predicted Mask\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Main Function\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # Define Your Directories Here\n",
    "    # ------------------------------\n",
    "    # Training and Validation directories (split from the same folder)\n",
    "    images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_train/images/\"\n",
    "    masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_train/masks/\"\n",
    "\n",
    "    # Testing directories (completely separate)\n",
    "    test_images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_test/images/\"\n",
    "    test_masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_test/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size    = 8  # Adjust based on GPU memory\n",
    "    num_epochs    = 30\n",
    "    learning_rate = 1e-4\n",
    "    val_split     = 0.2\n",
    "    save_path     = \"best_model_resnet101.pth\"\n",
    "    patience      = 10\n",
    "    post_process_flag = False  # Set to True to apply post-processing if implemented\n",
    "\n",
    "    # ------------------------------\n",
    "    # Device Configuration\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Main] Using device: {device}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect Training File Lists\n",
    "    # ------------------------------\n",
    "    # List all training image files\n",
    "    all_train_images = sorted([\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_images)} training image files in {images_dir}\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training image files found. Check your training path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding mask files exist\n",
    "    all_train_images = sorted([\n",
    "        f for f in all_train_images\n",
    "        if os.path.isfile(os.path.join(masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_train_images)} training images have corresponding masks.\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training mask files found or mismatched filenames. Check your training mask path!\")\n",
    "        return\n",
    "\n",
    "    # List all test image files\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(test_images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_images)} test image files in {test_images_dir}\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test image files found. Check your test path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding test mask files exist\n",
    "    all_test_images = sorted([\n",
    "        f for f in all_test_images\n",
    "        if os.path.isfile(os.path.join(test_masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_test_images)} test images have corresponding masks.\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test mask files found or mismatched filenames. Check your test mask path!\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------\n",
    "    # Train/Validation Split\n",
    "    # ------------------------------\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_train_images,\n",
    "        test_size=val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"[Main] Training samples: {len(train_files)}\")\n",
    "    print(f\"[Main] Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Datasets with Transforms\n",
    "    # ------------------------------\n",
    "    train_transform = ResizeAndToTensor(size=(512, 512))\n",
    "    val_transform = ResizeAndToTensor(size=(512, 512))\n",
    "    test_transform = ResizeAndToTensor(size=(512, 512))\n",
    "\n",
    "    train_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=train_files,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=val_files,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = BinarySegDataset(\n",
    "        images_dir=test_images_dir,\n",
    "        masks_dir=test_masks_dir,\n",
    "        file_list=all_test_images,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create DataLoaders\n",
    "    # ------------------------------\n",
    "    num_workers = 8  # Adjust based on CPU cores\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Initialize Model, Loss, Optimizer, Scheduler\n",
    "    # ------------------------------\n",
    "    model = ResNetUNetWithTransformer(\n",
    "        n_classes=2, \n",
    "        encoder_name='resnet34', \n",
    "        pretrained=True, \n",
    "        transformer_layers=16, \n",
    "        transformer_heads=16,\n",
    "        dropout_p=0.5\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize Dice Loss\n",
    "    criterion = DiceLoss()\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "    # Initialize Metrics\n",
    "    metrics_val = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    metrics_test = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "\n",
    "    # Initialize Gradient Scaler for Mixed Precision\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, scaler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device, epoch, metrics_val)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Scheduler step\n",
    "        scheduler.step(epoch + val_split)  # Using epoch count including validation split\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\">> Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "            early_stopping.counter = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"[Main] Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n[Main] Training complete. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"[Main] Best model saved at: {save_path}\")\n",
    "\n",
    "    # Plot Training and Validation Loss\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs, train_losses, 'b', label='Training loss')\n",
    "    plt.plot(epochs, val_losses, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Testing\n",
    "    # ------------------------------\n",
    "    print(\"\\n>>> Loading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_metrics = test_segmentation(model, test_loader, device, metrics=metrics_test)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Visualization of Test Samples\n",
    "    # ------------------------------\n",
    "    if len(test_dataset) > 0:\n",
    "        print(\"\\n>>> Visualizing predictions on test samples...\")\n",
    "        visualize_predictions(model, test_dataset, device, num_samples=20, post_process_flag=post_process_flag)\n",
    "    else:\n",
    "        print(\"[Warning] No test samples available for visualization.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Run the Main Function\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models  # For ResNet encoder\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchmetrics\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# 0. Reproducibility (Optional)\n",
    "# ------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Ensures that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Dataset and Transforms\n",
    "# ------------------------------\n",
    "\n",
    "class ResizeAndToTensor:\n",
    "    \"\"\"\n",
    "    Custom transform to resize images and masks to a fixed size, apply controlled augmentations,\n",
    "    and convert them to tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(512, 512), augment=False):\n",
    "        self.size = size  # (height, width)\n",
    "        self.augment = augment\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL Image): Input image.\n",
    "            mask (np.array): Corresponding mask as a NumPy array.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor (torch.Tensor): Resized and normalized image tensor.\n",
    "            mask_tensor (torch.Tensor): Resized mask tensor (Long type).\n",
    "        \"\"\"\n",
    "        # Resize image using bilinear interpolation\n",
    "        image = resize(\n",
    "            image,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.BILINEAR\n",
    "        )\n",
    "        # Resize mask using nearest-neighbor interpolation to preserve label integrity\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        mask_pil = resize(\n",
    "            mask_pil,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.NEAREST\n",
    "        )\n",
    "        # Convert back to NumPy array\n",
    "        mask = np.array(mask_pil, dtype=np.uint8)\n",
    "        \n",
    "        # Controlled Augmentations\n",
    "        if self.augment:\n",
    "            # Random brightness adjustment\n",
    "            enhancer = ImageEnhance.Brightness(image)\n",
    "            brightness_factor = random.uniform(0.8, 1.2)\n",
    "            image = enhancer.enhance(brightness_factor)\n",
    "            \n",
    "            # Random contrast adjustment\n",
    "            enhancer = ImageEnhance.Contrast(image)\n",
    "            contrast_factor = random.uniform(0.8, 1.2)\n",
    "            image = enhancer.enhance(contrast_factor)\n",
    "            \n",
    "            # Additional augmentations can be added here\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image_tensor = to_tensor(image)             # [C, H, W], float32 in [0,1]\n",
    "        image_tensor = self.normalize(image_tensor)\n",
    "        mask_tensor  = torch.from_numpy(mask).long()# [H, W], dtype=torch.int64\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "class BinarySegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for binary image segmentation.\n",
    "    Assumes that mask files are stored as .npy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, file_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with input images.\n",
    "            masks_dir (str): Directory with corresponding mask files (.npy).\n",
    "            file_list (list): List of image filenames.\n",
    "            transform (callable, optional): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir  = masks_dir\n",
    "        self.file_list  = file_list\n",
    "        self.transform  = transform\n",
    "\n",
    "        # Debug: Print how many files in this split\n",
    "        print(f\"[Dataset] Initialized with {len(file_list)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask pair at the specified index.\n",
    "        \"\"\"\n",
    "        img_filename = self.file_list[idx]\n",
    "        base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"[Error] Image file not found: {img_path}\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.masks_dir, base_name + \".npy\")\n",
    "        if not os.path.isfile(mask_path):\n",
    "            raise FileNotFoundError(f\"[Error] Mask file not found: {mask_path}\")\n",
    "        mask = np.load(mask_path)\n",
    "\n",
    "        # Remap label >1 to 1 to ensure binary segmentation\n",
    "        mask = np.where(mask > 1, 1, mask).astype(np.uint8)\n",
    "\n",
    "        # Apply transform\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Attention Gates Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Gate as described in \"Attention U-Net: Learning Where to Look for the Pancreas\"\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        # g: gating signal (from decoder)\n",
    "        # x: skip connection (from encoder)\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Transformer Module Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512*512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.position_embeddings = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
    "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [B, N, D]\n",
    "        return x + self.position_embeddings[:, :x.size(1), :]\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified Swin Transformer Block for demonstration purposes.\n",
    "    For full implementation, consider using a library like timm.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, window_size=7, shift_size=0, mlp_ratio=4., dropout=0.):\n",
    "        super(SwinTransformerBlock, self).__init__()\n",
    "        # Placeholder for a Swin Transformer block\n",
    "        # In practice, use a robust implementation from a library\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, int(embed_dim * mlp_ratio)),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(int(embed_dim * mlp_ratio), embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [B, N, D]\n",
    "        residual = x\n",
    "        x = self.layer_norm1(x)\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = residual + x\n",
    "        residual = x\n",
    "        x = self.layer_norm2(x)\n",
    "        x = self.mlp(x)\n",
    "        x = residual + x\n",
    "        return x\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    def __init__(self, embed_dim=2048, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super(TransformerBottleneck, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_encoding = PositionalEncoding(d_model=embed_dim)\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            SwinTransformerBlock(embed_dim, num_heads, dropout=dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: [B, C, H, W]\n",
    "        B, C, H, W = x.size()\n",
    "        N = H * W\n",
    "        # Flatten spatial dimensions\n",
    "        x = x.view(B, C, N).permute(0, 2, 1)  # [B, N, C]\n",
    "        x = self.positional_encoding(x)        # [B, N, C]\n",
    "        for block in self.transformer_blocks:\n",
    "            x = block(x)                        # [B, N, C]\n",
    "        x = self.layer_norm(x)                 # [B, N, C]\n",
    "        # Reshape back to [B, C, H, W]\n",
    "        x = x.permute(0, 2, 1).view(B, C, H, W)  # [B, C, H, W]\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# 4. U-Net Model Definition with Transformer and Attention Gates\n",
    "# ------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two convolutional layers each followed by BatchNorm, ReLU, and Dropout.\n",
    "    Includes a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_p=0.5):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "        )\n",
    "        # Residual connection\n",
    "        if in_channels != out_channels:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.residual(x)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for binary segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, logits, true):\n",
    "        probs = nn.functional.softmax(logits, dim=1)\n",
    "        true_one_hot = nn.functional.one_hot(true, num_classes=probs.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(probs * true_one_hot, dims)\n",
    "        cardinality = torch.sum(probs + true_one_hot, dims)\n",
    "        dice = (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Dice Loss and Cross-Entropy Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, smooth=1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice = DiceLoss(smooth)\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "    \n",
    "    def forward(self, logits, true):\n",
    "        return self.ce(logits, true) + self.dice(logits, true)\n",
    "\n",
    "class ResNetUNetWithTransformerAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net architecture with ResNet encoder, Transformer bottleneck, and Attention Gates.\n",
    "    Includes residual connections in the decoder's DoubleConv blocks.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_classes=2, encoder_name='resnet101', pretrained=True, transformer_layers=6, transformer_heads=8, dropout_p=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_classes (int): Number of output classes.\n",
    "            encoder_name (str): Name of the ResNet encoder to use ('resnet18', 'resnet34', 'resnet50', etc.).\n",
    "            pretrained (bool): Whether to use pretrained ResNet weights.\n",
    "            transformer_layers (int): Number of Transformer encoder layers.\n",
    "            transformer_heads (int): Number of attention heads in Transformer.\n",
    "            dropout_p (float): Dropout probability.\n",
    "        \"\"\"\n",
    "        super(ResNetUNetWithTransformerAttention, self).__init__()\n",
    "        \n",
    "        # Initialize ResNet encoder\n",
    "        if encoder_name == 'resnet18':\n",
    "            self.encoder = models.resnet18(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet34':\n",
    "            self.encoder = models.resnet34(pretrained=pretrained)\n",
    "            encoder_channels = [64, 64, 128, 256, 512]\n",
    "        elif encoder_name == 'resnet50':\n",
    "            self.encoder = models.resnet50(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet101':\n",
    "            self.encoder = models.resnet101(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        elif encoder_name == 'resnet152':\n",
    "            self.encoder = models.resnet152(pretrained=pretrained)\n",
    "            encoder_channels = [64, 256, 512, 1024, 2048]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported ResNet variant\")\n",
    "\n",
    "        # Encoder layers\n",
    "        self.initial = nn.Sequential(\n",
    "            self.encoder.conv1,  # [B, 64, H/2, W/2]\n",
    "            self.encoder.bn1,\n",
    "            self.encoder.relu,\n",
    "            self.encoder.maxpool  # [B, 64, H/4, W/4]\n",
    "        )\n",
    "        self.encoder_layer1 = self.encoder.layer1  # [B, 256, H/4, W/4] for ResNet101\n",
    "        self.encoder_layer2 = self.encoder.layer2  # [B, 512, H/8, W/8]\n",
    "        self.encoder_layer3 = self.encoder.layer3  # [B, 1024, H/16, W/16]\n",
    "        self.encoder_layer4 = self.encoder.layer4  # [B, 2048, H/32, W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        self.transformer = TransformerBottleneck(\n",
    "            embed_dim=encoder_channels[4],\n",
    "            num_heads=transformer_heads,\n",
    "            num_layers=transformer_layers,\n",
    "            dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Decoder layers with Attention Gates\n",
    "        self.up4 = nn.ConvTranspose2d(encoder_channels[4], encoder_channels[3], kernel_size=2, stride=2)\n",
    "        self.att4 = AttentionGate(F_g=encoder_channels[3], F_l=encoder_channels[3], F_int=encoder_channels[2])\n",
    "        self.conv4 = DoubleConv(encoder_channels[3] + encoder_channels[3], encoder_channels[3], dropout_p=dropout_p)\n",
    "\n",
    "        self.up3 = nn.ConvTranspose2d(encoder_channels[3], encoder_channels[2], kernel_size=2, stride=2)\n",
    "        self.att3 = AttentionGate(F_g=encoder_channels[2], F_l=encoder_channels[2], F_int=encoder_channels[1])\n",
    "        self.conv3 = DoubleConv(encoder_channels[2] + encoder_channels[2], encoder_channels[2], dropout_p=dropout_p)\n",
    "\n",
    "        self.up2 = nn.ConvTranspose2d(encoder_channels[2], encoder_channels[1], kernel_size=2, stride=2)\n",
    "        self.att2 = AttentionGate(F_g=encoder_channels[1], F_l=encoder_channels[1], F_int=encoder_channels[0])\n",
    "        self.conv2 = DoubleConv(encoder_channels[1] + encoder_channels[1], encoder_channels[1], dropout_p=dropout_p)\n",
    "\n",
    "        self.up1 = nn.ConvTranspose2d(encoder_channels[1], encoder_channels[0], kernel_size=1, stride=1)\n",
    "        self.att1 = AttentionGate(F_g=encoder_channels[0], F_l=encoder_channels[0], F_int=encoder_channels[0]//2)\n",
    "        self.conv1 = DoubleConv(encoder_channels[0] + encoder_channels[0], encoder_channels[0], dropout_p=dropout_p)\n",
    "\n",
    "        # Output convolution\n",
    "        self.out_conv = nn.Conv2d(encoder_channels[0], n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.initial(x)           # [B,64,H/4,W/4]\n",
    "        x1 = self.encoder_layer1(x0)   # [B,256,H/4,W/4] for ResNet101\n",
    "        x2 = self.encoder_layer2(x1)   # [B,512,H/8,W/8]\n",
    "        x3 = self.encoder_layer3(x2)   # [B,1024,H/16,W/16]\n",
    "        x4 = self.encoder_layer4(x3)   # [B,2048,H/32,W/32]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        x4 = self.transformer(x4)      # [B,2048,H/32,W/32]\n",
    "\n",
    "        # Decoder\n",
    "        up4 = self.up4(x4)             # [B,1024,H/16,W/16]\n",
    "        att4 = self.att4(g=up4, x=x3)   # [B,1024,H/16,W/16]\n",
    "        merge4 = torch.cat([up4, att4], dim=1)  # [B,2048,H/16,W/16]\n",
    "        conv4 = self.conv4(merge4)     # [B,1024,H/16,W/16]\n",
    "\n",
    "        up3 = self.up3(conv4)          # [B,512,H/8,W/8]\n",
    "        att3 = self.att3(g=up3, x=x2)   # [B,512,H/8,W/8]\n",
    "        merge3 = torch.cat([up3, att3], dim=1)  # [B,1024,H/8,W/8]\n",
    "        conv3 = self.conv3(merge3)     # [B,512,H/8,W/8]\n",
    "\n",
    "        up2 = self.up2(conv3)          # [B,256,H/4,W/4]\n",
    "        att2 = self.att2(g=up2, x=x1)   # [B,256,H/4,W/4]\n",
    "        merge2 = torch.cat([up2, att2], dim=1)  # [B,512,H/4,W/4]\n",
    "        conv2 = self.conv2(merge2)     # [B,256,H/4,W/4]\n",
    "\n",
    "        up1 = self.up1(conv2)          # [B,64,H/4,W/4]\n",
    "        att1 = self.att1(g=up1, x=x0)   # [B,64,H/4,W/4]\n",
    "        merge1 = torch.cat([up1, att1], dim=1)  # [B,128,H/4,W/4]\n",
    "        conv1 = self.conv1(merge1)     # [B,64,H/4,W/4]\n",
    "\n",
    "        # Output convolution\n",
    "        out = self.out_conv(conv1)     # [B,2,H/4,W/4]\n",
    "\n",
    "        # Upsample to original size\n",
    "        out = nn.functional.interpolate(out, scale_factor=4, mode='bilinear', align_corners=True)  # [B,2,H,W]\n",
    "\n",
    "        return out\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Early Stopping Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Initial best loss: {self.best_loss:.4f}\")\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"[EarlyStopping] Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Validation loss improved to: {self.best_loss:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Training and Validation Functions\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch_idx, scaler, clip_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using Mixed Precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        criterion (Loss): Combined loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        scaler (torch.cuda.amp.GradScaler): Gradient scaler for mixed precision.\n",
    "        clip_grad_norm (float): Maximum norm for gradient clipping.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Train] Starting epoch {epoch_idx+1} ---\")\n",
    "    # Use tqdm to show progress\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Train] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Train] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    return average_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, epoch_idx, metrics=None):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch using Mixed Precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (Loss): Combined loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics to compute.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Val] Starting epoch {epoch_idx+1} ---\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Val] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Val] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    \n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        metric_values = {}\n",
    "    \n",
    "    # Optionally, print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Val] Epoch {epoch_idx+1} Metrics: {metric_str} ---\")\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Testing and Metrics\n",
    "# ------------------------------\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient for binary masks.\n",
    "    \n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted mask [H, W].\n",
    "        target (torch.Tensor): Ground truth mask [H, W].\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    \n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1).float()\n",
    "    target_flat = target.view(-1).float()\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def test_segmentation(model, loader, device, metrics=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and computes the average Dice coefficient and other metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): Device to run on.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of average metrics on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Testing\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)  # [B, H, W]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)    # [B, n_classes, H, W]\n",
    "                preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        # If metrics are not provided, compute only Dice\n",
    "        dice_scores = []\n",
    "        for pred, mask in zip(preds, masks):\n",
    "            dice = dice_coefficient(pred, mask)\n",
    "            dice_scores.append(dice)\n",
    "        metric_values = {\n",
    "            'dice': np.mean(dice_scores) if dice_scores else 0\n",
    "        }\n",
    "\n",
    "    # Print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Test] Metrics: {metric_str} ---\")\n",
    "    else:\n",
    "        print(f\"--- [Test] Dice Coefficient: {metric_values['dice']:.4f} ---\")\n",
    "\n",
    "    return metric_values\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Visualization of Predictions\n",
    "# ------------------------------\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=3, post_process_flag=False):\n",
    "    \"\"\"\n",
    "    Visualizes predictions alongside the original images and ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        dataset (Dataset): The test dataset.\n",
    "        device (torch.device): Device to run on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        post_process_flag (bool): Whether to apply post-processing to predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_batch = image.unsqueeze(0).to(device, non_blocking=True)  # [1, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(image_batch)  # [1, n_classes, H, W]\n",
    "                pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Optional Post-Processing\n",
    "        if post_process_flag:\n",
    "            # Example: Simple morphological operations can be implemented here\n",
    "            # For demonstration, this is left as a placeholder\n",
    "            pass\n",
    "\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "        mask_np = mask.cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Handle image normalization for visualization\n",
    "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "        image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(mask_np, cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(pred, cmap='gray')\n",
    "        axs[2].set_title(\"Predicted Mask\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Main Function\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # Define Your Directories Here\n",
    "    # ------------------------------\n",
    "    # Training and Validation directories (split from the same folder)\n",
    "    images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_train/images/\"\n",
    "    masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_train/masks/\"\n",
    "\n",
    "    # Testing directories (completely separate)\n",
    "    test_images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_test/images/\"\n",
    "    test_masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_test/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size    = 8  # Adjust based on GPU memory\n",
    "    num_epochs    = 50\n",
    "    learning_rate = 1e-4\n",
    "    val_split     = 0.2\n",
    "    save_path     = \"best_model_resnet101_attention.pth\"\n",
    "    patience      = 15\n",
    "    post_process_flag = False  # Set to True to apply post-processing if implemented\n",
    "\n",
    "    # ------------------------------\n",
    "    # Device Configuration\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Main] Using device: {device}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect Training File Lists\n",
    "    # ------------------------------\n",
    "    # List all training image files\n",
    "    all_train_images = sorted([\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_images)} training image files in {images_dir}\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training image files found. Check your training path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding mask files exist\n",
    "    all_train_images = sorted([\n",
    "        f for f in all_train_images\n",
    "        if os.path.isfile(os.path.join(masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_train_images)} training images have corresponding masks.\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training mask files found or mismatched filenames. Check your training mask path!\")\n",
    "        return\n",
    "\n",
    "    # List all test image files\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(test_images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_images)} test image files in {test_images_dir}\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test image files found. Check your test path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding test mask files exist\n",
    "    all_test_images = sorted([\n",
    "        f for f in all_test_images\n",
    "        if os.path.isfile(os.path.join(test_masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_test_images)} test images have corresponding masks.\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test mask files found or mismatched filenames. Check your test mask path!\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------\n",
    "    # Train/Validation Split\n",
    "    # ------------------------------\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_train_images,\n",
    "        test_size=val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"[Main] Training samples: {len(train_files)}\")\n",
    "    print(f\"[Main] Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Datasets with Transforms\n",
    "    # ------------------------------\n",
    "    train_transform = ResizeAndToTensor(size=(512, 512), augment=True)\n",
    "    val_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "    test_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "\n",
    "    train_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=train_files,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=val_files,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = BinarySegDataset(\n",
    "        images_dir=test_images_dir,\n",
    "        masks_dir=test_masks_dir,\n",
    "        file_list=all_test_images,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create DataLoaders\n",
    "    # ------------------------------\n",
    "    num_workers = 8  # Adjust based on CPU cores\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Initialize Model, Loss, Optimizer, Scheduler\n",
    "    # ------------------------------\n",
    "    model = ResNetUNetWithTransformerAttention(\n",
    "        n_classes=2, \n",
    "        encoder_name='resnet34', \n",
    "        pretrained=True, \n",
    "        transformer_layers=12, \n",
    "        transformer_heads=16,\n",
    "        dropout_p=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize Combined Loss (Dice + Cross-Entropy)\n",
    "    # Without class weights\n",
    "    criterion = CombinedLoss(weight=None)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    # Scheduler: Cosine Annealing\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    # Initialize Metrics\n",
    "    metrics_val = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2).to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "    \n",
    "    metrics_test = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2).to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "    \n",
    "    # Initialize Gradient Scaler for Mixed Precision\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, scaler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device, epoch, metrics_val)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Scheduler step based on epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\">> Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "            early_stopping.counter = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"[Main] Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n[Main] Training complete. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"[Main] Best model saved at: {save_path}\")\n",
    "\n",
    "    # Plot Training and Validation Loss\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs_range, train_losses, 'b', label='Training loss')\n",
    "    plt.plot(epochs_range, val_losses, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Testing\n",
    "    # ------------------------------\n",
    "    print(\"\\n>>> Loading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_metrics = test_segmentation(model, test_loader, device, metrics=metrics_test)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Visualization of Test Samples\n",
    "    # ------------------------------\n",
    "    if len(test_dataset) > 0:\n",
    "        print(\"\\n>>> Visualizing predictions on test samples...\")\n",
    "        visualize_predictions(model, test_dataset, device, num_samples=20, post_process_flag=post_process_flag)\n",
    "    else:\n",
    "        print(\"[Warning] No test samples available for visualization.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Run the Main Function\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision.transforms.functional import resize, to_tensor\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "import torchmetrics\n",
    "import random\n",
    "\n",
    "# ------------------------------\n",
    "# 0. Reproducibility (Optional)\n",
    "# ------------------------------\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Sets the seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    # Ensures that CUDA operations are deterministic\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Combined Loss Function\n",
    "# ------------------------------\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Dice Loss for binary segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, smooth=1):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, logits, true):\n",
    "        probs = nn.functional.softmax(logits, dim=1)\n",
    "        true_one_hot = nn.functional.one_hot(true, num_classes=probs.shape[1]).permute(0, 3, 1, 2).float()\n",
    "        dims = (0, 2, 3)\n",
    "        intersection = torch.sum(probs * true_one_hot, dims)\n",
    "        cardinality = torch.sum(probs + true_one_hot, dims)\n",
    "        dice = (2. * intersection + self.smooth) / (cardinality + self.smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Dice Loss and Cross-Entropy Loss.\n",
    "    \"\"\"\n",
    "    def __init__(self, weight=None, smooth=1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.dice = DiceLoss(smooth)\n",
    "        self.ce = nn.CrossEntropyLoss(weight=weight)\n",
    "    \n",
    "    def forward(self, logits, true):\n",
    "        return self.ce(logits, true) + self.dice(logits, true)\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Dataset and Transforms\n",
    "# ------------------------------\n",
    "\n",
    "class ResizeAndToTensor:\n",
    "    \"\"\"\n",
    "    Custom transform to resize images and masks to a fixed size, apply controlled augmentations,\n",
    "    and convert them to tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, size=(512, 512), augment=False):\n",
    "        self.size = size  # (height, width)\n",
    "        self.augment = augment\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def __call__(self, image, mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (PIL Image): Input image.\n",
    "            mask (np.array): Corresponding mask as a NumPy array.\n",
    "        \n",
    "        Returns:\n",
    "            image_tensor (torch.Tensor): Resized and normalized image tensor.\n",
    "            mask_tensor (torch.Tensor): Resized mask tensor (Long type).\n",
    "        \"\"\"\n",
    "        # Resize image using bilinear interpolation\n",
    "        image = resize(\n",
    "            image,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.BILINEAR\n",
    "        )\n",
    "        # Resize mask using nearest-neighbor interpolation to preserve label integrity\n",
    "        mask_pil = Image.fromarray(mask)\n",
    "        mask_pil = resize(\n",
    "            mask_pil,\n",
    "            self.size,\n",
    "            interpolation=InterpolationMode.NEAREST\n",
    "        )\n",
    "        # Convert back to NumPy array\n",
    "        mask = np.array(mask_pil, dtype=np.uint8)\n",
    "        \n",
    "        # Controlled Augmentations\n",
    "        if self.augment:\n",
    "            # Random brightness adjustment\n",
    "            enhancer = ImageEnhance.Brightness(image)\n",
    "            brightness_factor = random.uniform(0.8, 1.2)\n",
    "            image = enhancer.enhance(brightness_factor)\n",
    "            \n",
    "            # Random contrast adjustment\n",
    "            enhancer = ImageEnhance.Contrast(image)\n",
    "            contrast_factor = random.uniform(0.8, 1.2)\n",
    "            image = enhancer.enhance(contrast_factor)\n",
    "            \n",
    "            # Additional augmentations can be added here\n",
    "\n",
    "        # Convert image and mask to tensors\n",
    "        image_tensor = to_tensor(image)             # [C, H, W], float32 in [0,1]\n",
    "        image_tensor = self.normalize(image_tensor)\n",
    "        mask_tensor  = torch.from_numpy(mask).long()# [H, W], dtype=torch.int64\n",
    "\n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "class BinarySegDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for binary image segmentation.\n",
    "    Assumes that mask files are stored as .npy files.\n",
    "    \"\"\"\n",
    "    def __init__(self, images_dir, masks_dir, file_list, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images_dir (str): Directory with input images.\n",
    "            masks_dir (str): Directory with corresponding mask files (.npy).\n",
    "            file_list (list): List of image filenames.\n",
    "            transform (callable, optional): Transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir  = masks_dir\n",
    "        self.file_list  = file_list\n",
    "        self.transform  = transform\n",
    "\n",
    "        # Debug: Print how many files in this split\n",
    "        print(f\"[Dataset] Initialized with {len(file_list)} files.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the image and mask pair at the specified index.\n",
    "        \"\"\"\n",
    "        img_filename = self.file_list[idx]\n",
    "        base_name = os.path.splitext(img_filename)[0]\n",
    "\n",
    "        # Load image\n",
    "        img_path = os.path.join(self.images_dir, img_filename)\n",
    "        if not os.path.isfile(img_path):\n",
    "            raise FileNotFoundError(f\"[Error] Image file not found: {img_path}\")\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        # Load mask\n",
    "        mask_path = os.path.join(self.masks_dir, base_name + \".npy\")\n",
    "        if not os.path.isfile(mask_path):\n",
    "            raise FileNotFoundError(f\"[Error] Mask file not found: {mask_path}\")\n",
    "        mask = np.load(mask_path)\n",
    "\n",
    "        # Remap label >1 to 1 to ensure binary segmentation\n",
    "        mask = np.where(mask > 1, 1, mask).astype(np.uint8)\n",
    "\n",
    "        # Apply transform\n",
    "        if self.transform is not None:\n",
    "            image, mask = self.transform(image, mask)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Attention Gates Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Gate as described in \"Attention U-Net: Learning Where to Look for the Pancreas\"\n",
    "    \"\"\"\n",
    "    def __init__(self, F_g, F_l, F_int):\n",
    "        super(AttentionGate, self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "        \n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "    \n",
    "    def forward(self, g, x):\n",
    "        # g: gating signal (from decoder)\n",
    "        # x: skip connection (from encoder)\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1 + x1)\n",
    "        psi = self.psi(psi)\n",
    "        return x * psi\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Custom Transformer Bottleneck Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class TransformerBottleneck(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Transformer Bottleneck for U-Net++ architecture.\n",
    "    Utilizes standard TransformerEncoder layers to capture global dependencies.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=512, num_heads=8, num_layers=6, dropout=0.1):\n",
    "        super(TransformerBottleneck, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.flatten = nn.Flatten(2)  # [B, C, H, W] -> [B, C, H*W]\n",
    "        self.pos_embedding = nn.Parameter(torch.zeros(1, embed_dim, 32*32))  # [1, 512, 1024]\n",
    "        nn.init.trunc_normal_(self.pos_embedding, std=0.02)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dropout=dropout,\n",
    "            activation='gelu'\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.unflatten = nn.Unflatten(2, (32, 32))  # [B, 512, 1024] -> [B, 512, 32, 32]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, C, H, W] where H=32, W=32\n",
    "        B, C, H, W = x.size()\n",
    "        x = self.flatten(x)  # [B, C, H*W] = [B, 512, 1024]\n",
    "        x = x + self.pos_embedding  # [B, 512, 1024]\n",
    "        x = x.permute(0, 2, 1)  # [B, 1024, 512]\n",
    "        x = self.transformer_encoder(x)  # [B, 1024, 512]\n",
    "        x = x.permute(0, 2, 1)  # [B, 512, 1024]\n",
    "        x = self.unflatten(x)    # [B, 512, 32, 32]\n",
    "        return x\n",
    "\n",
    "# ------------------------------\n",
    "# 5. U-Net++ Model Definition with Transformer and Attention Gates\n",
    "# ------------------------------\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"\n",
    "    A block consisting of two convolutional layers each followed by BatchNorm, ReLU, and Dropout.\n",
    "    Includes a residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, dropout_p=0.5):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=dropout_p),\n",
    "        )\n",
    "        # Residual connection\n",
    "        if in_channels != out_channels:\n",
    "            self.residual = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        else:\n",
    "            self.residual = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + self.residual(x)\n",
    "\n",
    "class UNetPlusPlus(nn.Module):\n",
    "    \"\"\"\n",
    "    U-Net++ architecture with nested skip connections and a Transformer bottleneck.\n",
    "    Includes Attention Gates and residual connections.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=2, dropout_p=0.5):\n",
    "        super(UNetPlusPlus, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = DoubleConv(in_channels, 64, dropout_p)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc2 = DoubleConv(64, 128, dropout_p)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc3 = DoubleConv(128, 256, dropout_p)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.enc4 = DoubleConv(256, 512, dropout_p)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        self.transformer = TransformerBottleneck(\n",
    "            embed_dim=512,\n",
    "            num_heads=16,\n",
    "            num_layers=16,\n",
    "            dropout=0.1\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        # Level 4\n",
    "        self.up4_0 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.att4_0 = AttentionGate(F_g=256, F_l=512, F_int=128)  # F_l corrected to 512\n",
    "        self.dec4_0 = DoubleConv(256 + 512, 256, dropout_p)      # Merge up4_0 and att4_0\n",
    "\n",
    "        # Level 3\n",
    "        self.up3_0 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.att3_0 = AttentionGate(F_g=128, F_l=256, F_int=64)   # F_l corrected to 256\n",
    "        self.dec3_0 = DoubleConv(128 + 256, 128, dropout_p)      # Merge up3_0 and att3_0\n",
    "\n",
    "        # Level 2\n",
    "        self.up2_0 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.att2_0 = AttentionGate(F_g=64, F_l=128, F_int=32)    # F_l corrected to 128\n",
    "        self.dec2_0 = DoubleConv(64 + 128, 64, dropout_p)        # Merge up2_0 and att2_0\n",
    "\n",
    "        # Final Output\n",
    "        self.final_conv = nn.Conv2d(64 + 64, out_channels, kernel_size=1)  # Merge up1_0 and enc1\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc1(x)          # [B,64,H,W]\n",
    "        pool1 = self.pool1(enc1)     # [B,64,H/2,W/2]\n",
    "\n",
    "        enc2 = self.enc2(pool1)      # [B,128,H/2,W/2]\n",
    "        pool2 = self.pool2(enc2)     # [B,128,H/4,W/4]\n",
    "\n",
    "        enc3 = self.enc3(pool2)      # [B,256,H/4,W/4]\n",
    "        pool3 = self.pool3(enc3)     # [B,256,H/8,W/8]\n",
    "\n",
    "        enc4 = self.enc4(pool3)      # [B,512,H/8,W/8]\n",
    "        pool4 = self.pool4(enc4)     # [B,512,H/16,W/16]\n",
    "\n",
    "        # Transformer Bottleneck\n",
    "        bottleneck = self.transformer(pool4)  # [B,512,H/16,W/16]\n",
    "\n",
    "        # Decoder\n",
    "        up4_0 = self.up4_0(bottleneck)        # [B,256,H/8,W/8]\n",
    "        att4_0 = self.att4_0(up4_0, enc4)      # [B,256,H/8,W/8]\n",
    "        merge4_0 = torch.cat([up4_0, att4_0], dim=1)  # [B,256+256=512,H/8,W/8]\n",
    "        dec4_0 = self.dec4_0(merge4_0)         # [B,256,H/8,W/8]\n",
    "\n",
    "        up3_0 = self.up3_0(dec4_0)            # [B,128,H/4,W/4]\n",
    "        att3_0 = self.att3_0(up3_0, enc3)      # [B,128,H/4,W/4]\n",
    "        merge3_0 = torch.cat([up3_0, att3_0], dim=1)  # [B,128+128=256,H/4,W/4]\n",
    "        dec3_0 = self.dec3_0(merge3_0)         # [B,128,H/4,W/4]\n",
    "\n",
    "        up2_0 = self.up2_0(dec3_0)            # [B,64,H/2,W/2]\n",
    "        att2_0 = self.att2_0(up2_0, enc2)      # [B,64,H/2,W/2]\n",
    "        merge2_0 = torch.cat([up2_0, att2_0], dim=1)  # [B,64+64=128,H/2,W/2]\n",
    "        dec2_0 = self.dec2_0(merge2_0)         # [B,64,H/2,W/2]\n",
    "\n",
    "        # Final upsampling\n",
    "        up1_0 = nn.functional.interpolate(dec2_0, scale_factor=2, mode='bilinear', align_corners=True)  # [B,64,H,W]\n",
    "        merge1_0 = torch.cat([up1_0, enc1], dim=1)  # [B,64+64=128,H,W]\n",
    "        final = self.final_conv(merge1_0)            # [B,2,H,W]\n",
    "\n",
    "        return final\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Early Stopping Implementation\n",
    "# ------------------------------\n",
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stops the training if validation loss doesn't improve after a given patience.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0.0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.delta = delta\n",
    "        self.best_loss = None\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Initial best loss: {self.best_loss:.4f}\")\n",
    "        elif val_loss > self.best_loss - self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] No improvement. Counter: {self.counter}/{self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if self.verbose:\n",
    "                    print(\"[EarlyStopping] Early stopping triggered.\")\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                print(f\"[EarlyStopping] Validation loss improved to: {self.best_loss:.4f}\")\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Training and Validation Functions\n",
    "# ------------------------------\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device, epoch_idx, scaler, clip_grad_norm=1.0):\n",
    "    \"\"\"\n",
    "    Trains the model for one epoch using Mixed Precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the training set.\n",
    "        optimizer (Optimizer): Optimizer.\n",
    "        criterion (Loss): Combined loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        scaler (torch.cuda.amp.GradScaler): Gradient scaler for mixed precision.\n",
    "        clip_grad_norm (float): Maximum norm for gradient clipping.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average training loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Train] Starting epoch {epoch_idx+1} ---\")\n",
    "    # Use tqdm to show progress\n",
    "    for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Training\", leave=False)):\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        masks = masks.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=clip_grad_norm)\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"[Train] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Train] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    return average_loss\n",
    "\n",
    "def validate_one_epoch(model, loader, criterion, device, epoch_idx, metrics=None):\n",
    "    \"\"\"\n",
    "    Validates the model for one epoch using Mixed Precision.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The segmentation model.\n",
    "        loader (DataLoader): DataLoader for the validation set.\n",
    "        criterion (Loss): Combined loss function.\n",
    "        device (torch.device): Device to run on.\n",
    "        epoch_idx (int): Current epoch index.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics to compute.\n",
    "    \n",
    "    Returns:\n",
    "        float: Average validation loss for the epoch.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    print(f\"--- [Val] Starting epoch {epoch_idx+1} ---\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Validation\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item() * images.size(0)\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"[Val] Batch {batch_idx}, Loss: {loss.item():.4f}\")\n",
    "            \n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    average_loss = total_loss / len(loader.dataset)\n",
    "    print(f\"--- [Val] Epoch {epoch_idx+1} Average Loss: {average_loss:.4f} ---\")\n",
    "    \n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        metric_values = {}\n",
    "    \n",
    "    # Optionally, print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Val] Epoch {epoch_idx+1} Metrics: {metric_str} ---\")\n",
    "    \n",
    "    return average_loss\n",
    "\n",
    "# ------------------------------\n",
    "# 8. Testing and Metrics\n",
    "# ------------------------------\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Computes the Dice coefficient for binary masks.\n",
    "    \n",
    "    Args:\n",
    "        pred (torch.Tensor): Predicted mask [H, W].\n",
    "        target (torch.Tensor): Ground truth mask [H, W].\n",
    "        smooth (float): Smoothing factor to avoid division by zero.\n",
    "    \n",
    "    Returns:\n",
    "        float: Dice coefficient.\n",
    "    \"\"\"\n",
    "    pred_flat = pred.view(-1).float()\n",
    "    target_flat = target.view(-1).float()\n",
    "    \n",
    "    intersection = (pred_flat * target_flat).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred_flat.sum() + target_flat.sum() + smooth)\n",
    "    return dice.item()\n",
    "\n",
    "def test_segmentation(model, loader, device, metrics=None):\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test set and computes the average Dice coefficient and other metrics.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        loader (DataLoader): DataLoader for the test set.\n",
    "        device (torch.device): Device to run on.\n",
    "        metrics (dict, optional): Dictionary of TorchMetrics.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of average metrics on the test set.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, masks) in enumerate(tqdm(loader, desc=\"Testing\", leave=False)):\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            masks = masks.to(device, non_blocking=True)  # [B, H, W]\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(images)    # [B, n_classes, H, W]\n",
    "                preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "\n",
    "            # Compute metrics\n",
    "            if metrics:\n",
    "                for metric in metrics.values():\n",
    "                    metric(preds, masks)\n",
    "\n",
    "    # Compute metric values\n",
    "    if metrics:\n",
    "        metric_values = {k: v.compute().item() for k, v in metrics.items()}\n",
    "        # Reset metrics\n",
    "        for metric in metrics.values():\n",
    "            metric.reset()\n",
    "    else:\n",
    "        # If metrics are not provided, compute only Dice\n",
    "        dice_scores = []\n",
    "        for pred, mask in zip(preds, masks):\n",
    "            dice = dice_coefficient(pred, mask)\n",
    "            dice_scores.append(dice)\n",
    "        metric_values = {\n",
    "            'dice': np.mean(dice_scores) if dice_scores else 0\n",
    "        }\n",
    "\n",
    "    # Print metrics\n",
    "    if metrics:\n",
    "        metric_str = \" | \".join([f\"{k}: {v:.4f}\" for k, v in metric_values.items()])\n",
    "        print(f\"--- [Test] Metrics: {metric_str} ---\")\n",
    "    else:\n",
    "        print(f\"--- [Test] Dice Coefficient: {metric_values['dice']:.4f} ---\")\n",
    "\n",
    "    return metric_values\n",
    "\n",
    "# ------------------------------\n",
    "# 9. Visualization of Predictions\n",
    "# ------------------------------\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=3, post_process_flag=False):\n",
    "    \"\"\"\n",
    "    Visualizes predictions alongside the original images and ground truth masks.\n",
    "    \n",
    "    Args:\n",
    "        model (nn.Module): The trained segmentation model.\n",
    "        dataset (Dataset): The test dataset.\n",
    "        device (torch.device): Device to run on.\n",
    "        num_samples (int): Number of samples to visualize.\n",
    "        post_process_flag (bool): Whether to apply post-processing to predictions.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        image_batch = image.unsqueeze(0).to(device, non_blocking=True)  # [1, C, H, W]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(image_batch)  # [1, n_classes, H, W]\n",
    "                pred = torch.argmax(output, dim=1).squeeze(0).cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Optional Post-Processing\n",
    "        if post_process_flag:\n",
    "            # Example: Simple morphological operations can be implemented here\n",
    "            # For demonstration, this is left as a placeholder\n",
    "            pass\n",
    "\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()  # [H, W, C]\n",
    "        mask_np = mask.cpu().numpy()  # [H, W]\n",
    "\n",
    "        # Handle image normalization for visualization\n",
    "        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])  # Unnormalize\n",
    "        image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "        # Plotting\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "        axs[0].imshow(image_np)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis(\"off\")\n",
    "\n",
    "        axs[1].imshow(mask_np, cmap='gray')\n",
    "        axs[1].set_title(\"Ground Truth Mask\")\n",
    "        axs[1].axis(\"off\")\n",
    "\n",
    "        axs[2].imshow(pred, cmap='gray')\n",
    "        axs[2].set_title(\"Predicted Mask\")\n",
    "        axs[2].axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# ------------------------------\n",
    "# 10. Main Function\n",
    "# ------------------------------\n",
    "\n",
    "def main():\n",
    "    # ------------------------------\n",
    "    # Define Your Directories Here\n",
    "    # ------------------------------\n",
    "    # Training and Validation directories (split from the same folder)\n",
    "    images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_train/images/\"\n",
    "    masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_train/masks/\"\n",
    "\n",
    "    # Testing directories (completely separate)\n",
    "    test_images_dir = \"/home/yanghehao/tracklearning/segmentation/phantom_test/images/\"\n",
    "    test_masks_dir  = \"/home/yanghehao/tracklearning/segmentation/phantom_test/masks/\"\n",
    "\n",
    "    # Hyperparameters\n",
    "    batch_size    = 8  # Adjust based on GPU memory\n",
    "    num_epochs    = 50\n",
    "    learning_rate = 1e-4\n",
    "    val_split     = 0.2\n",
    "    save_path     = \"best_model_unetpp_transformer.pth\"\n",
    "    patience      = 2\n",
    "    post_process_flag = False  # Set to True to apply post-processing if implemented\n",
    "\n",
    "    # ------------------------------\n",
    "    # Device Configuration\n",
    "    # ------------------------------\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[Main] Using device: {device}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Collect Training File Lists\n",
    "    # ------------------------------\n",
    "    # List all training image files\n",
    "    all_train_images = sorted([\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_train_images)} training image files in {images_dir}\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training image files found. Check your training path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding mask files exist\n",
    "    all_train_images = sorted([\n",
    "        f for f in all_train_images\n",
    "        if os.path.isfile(os.path.join(masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_train_images)} training images have corresponding masks.\")\n",
    "\n",
    "    if len(all_train_images) == 0:\n",
    "        print(\"[Error] No training mask files found or mismatched filenames. Check your training mask path!\")\n",
    "        return\n",
    "\n",
    "    # List all test image files\n",
    "    all_test_images = sorted([\n",
    "        f for f in os.listdir(test_images_dir)\n",
    "        if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n",
    "    ])\n",
    "    print(f\"[Main] Found {len(all_test_images)} test image files in {test_images_dir}\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test image files found. Check your test path!\")\n",
    "        return\n",
    "\n",
    "    # Ensure corresponding test mask files exist\n",
    "    all_test_images = sorted([\n",
    "        f for f in all_test_images\n",
    "        if os.path.isfile(os.path.join(test_masks_dir, os.path.splitext(f)[0] + \".npy\"))\n",
    "    ])\n",
    "    print(f\"[Main] {len(all_test_images)} test images have corresponding masks.\")\n",
    "\n",
    "    if len(all_test_images) == 0:\n",
    "        print(\"[Error] No test mask files found or mismatched filenames. Check your test mask path!\")\n",
    "        return\n",
    "\n",
    "    # ------------------------------\n",
    "    # Train/Validation Split\n",
    "    # ------------------------------\n",
    "    train_files, val_files = train_test_split(\n",
    "        all_train_images,\n",
    "        test_size=val_split,\n",
    "        random_state=42\n",
    "    )\n",
    "    print(f\"[Main] Training samples: {len(train_files)}\")\n",
    "    print(f\"[Main] Validation samples: {len(val_files)}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create Datasets with Transforms\n",
    "    # ------------------------------\n",
    "    train_transform = ResizeAndToTensor(size=(512, 512), augment=True)\n",
    "    val_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "    test_transform = ResizeAndToTensor(size=(512, 512), augment=False)\n",
    "\n",
    "    train_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=train_files,\n",
    "        transform=train_transform\n",
    "    )\n",
    "    val_dataset = BinarySegDataset(\n",
    "        images_dir=images_dir,\n",
    "        masks_dir=masks_dir,\n",
    "        file_list=val_files,\n",
    "        transform=val_transform\n",
    "    )\n",
    "    test_dataset = BinarySegDataset(\n",
    "        images_dir=test_images_dir,\n",
    "        masks_dir=test_masks_dir,\n",
    "        file_list=all_test_images,\n",
    "        transform=test_transform\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Create DataLoaders\n",
    "    # ------------------------------\n",
    "    num_workers = 8  # Adjust based on CPU cores\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True if torch.cuda.is_available() else False,\n",
    "        persistent_workers=True if torch.cuda.is_available() else False\n",
    "    )\n",
    "\n",
    "    # ------------------------------\n",
    "    # Initialize Model, Loss, Optimizer, Scheduler\n",
    "    # ------------------------------\n",
    "    model = UNetPlusPlus(\n",
    "        in_channels=3, \n",
    "        out_channels=2, \n",
    "        dropout_p=0.1\n",
    "    ).to(device)\n",
    "    \n",
    "    # Initialize Combined Loss (Dice + Cross-Entropy)\n",
    "    # Without class weights\n",
    "    criterion = CombinedLoss(weight=None)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    # Scheduler: Cosine Annealing\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Initialize EarlyStopping\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "    # Initialize Metrics\n",
    "    metrics_val = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "    \n",
    "    metrics_test = {\n",
    "        'dice': torchmetrics.Dice(num_classes=2, average='macro').to(device),\n",
    "        'iou': torchmetrics.JaccardIndex(task='binary').to(device),\n",
    "        'accuracy': torchmetrics.Accuracy(task='binary').to(device)\n",
    "    }\n",
    "    \n",
    "    # Initialize Gradient Scaler for Mixed Precision\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Training Loop\n",
    "    # ------------------------------\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, scaler)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # Validate\n",
    "        val_loss = validate_one_epoch(model, val_loader, criterion, device, epoch, metrics_val)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # Scheduler step based on epoch\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] | LR: {optimizer.param_groups[0]['lr']:.6f} | \"\n",
    "              f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Check for improvement\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\">> Saved best model with Val Loss: {best_val_loss:.4f}\")\n",
    "            early_stopping.counter = 0  # Reset early stopping counter\n",
    "        else:\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"[Main] Early stopping triggered. Stopping training.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n[Main] Training complete. Best Validation Loss: {best_val_loss:.4f}\")\n",
    "    print(f\"[Main] Best model saved at: {save_path}\")\n",
    "\n",
    "    # Plot Training and Validation Loss\n",
    "    epochs_range = range(1, len(train_losses) + 1)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(epochs_range, train_losses, 'b', label='Training loss')\n",
    "    plt.plot(epochs_range, val_losses, 'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # ------------------------------\n",
    "    # Testing\n",
    "    # ------------------------------\n",
    "    print(\"\\n>>> Loading best model for testing...\")\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    model.eval()\n",
    "\n",
    "    test_metrics = test_segmentation(model, test_loader, device, metrics=metrics_test)\n",
    "    print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "    # ------------------------------\n",
    "    # Visualization of Test Samples\n",
    "    # ------------------------------\n",
    "    if len(test_dataset) > 0:\n",
    "        print(\"\\n>>> Visualizing predictions on test samples...\")\n",
    "        visualize_predictions(model, test_dataset, device, num_samples=20, post_process_flag=post_process_flag)\n",
    "    else:\n",
    "        print(\"[Warning] No test samples available for visualization.\")\n",
    "\n",
    "# ------------------------------\n",
    "# 11. Run the Main Function\n",
    "# ------------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rr-0.4.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
